<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Neel Vora</title>

    <meta name="author" content="Neel Vora">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Neel Vora 
                </p>
                <p> I'm a computer science graduate pursuing Master's degree. I bring over three years of experince in 
                    field of applied machine learning and software engineering. Currently I am machine learning intern at 
                    <a href="https://www.lbl.gov/">Lawerence Berkeley National Laboratory</a> in Berkeley, where I work 
                    on optimizing quantum qubit readout through signal processing and noise reduction techniques.
                </p>
                <p>
                  I am currently working on my thesis under the guidance of <a href="https://people.cs.umass.edu/~phuc/">Dr. VP Nguyen</a> 
                  specializing in the field of machine learning on edge devices. 
                </p>
                <p style="text-align:center">
                  <a href="mailto:neelvora27@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="assets/neel's cv.pdf">Resume</a>&nbsp;/&nbsp;
                  <a href="https://github.com/freaksie/">Github</a>&nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/neelvora27">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=EV0TOzcAAAAJ&hl=en">Scholar</a>
                  
                </p>
                <br>
                <strong style="color: red; animation: blink 1s infinite; display: inline;">Looking for full-time opportunities in MLE or SDE roles!</strong>
                <style>
                  @keyframes blink {
                    0% { color: red; }
                    10% { color: transparent; }
                    100% { color: red; }
                  }
                </style>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="assets/photo.jpeg">
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <hr>
                <h2>Highlighted Works</h2>
                <p>
                  I'm interested in computer vision, machine learning, image and signal processing processing, edge computing. 
                  Most of my work is about development and deployment of end-to-end machine learning pipeline either on edge device or cloud servers.
                </p>
              </td>
            </tr>
          </tbody></table>  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <!-- <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/zipnerf.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/zipnerf.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function zipnerf_start() {
                    document.getElementById('zipnerf_image').style.opacity = "1";
                  }
      
                  function zipnerf_stop() {
                    document.getElementById('zipnerf_image').style.opacity = "0";
                  }
                  zipnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://jonbarron.info/zipnerf">
                  <span class="papertitle">Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="https://phogzone.com/">Peter Hedman</a>
                <br>
                <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Finalist)</strong></font>
                <br>
                <a href="http://jonbarron.info/zipnerf">project page</a>
                /
                <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a>
                /
                <a href="https://arxiv.org/abs/2304.06706">arXiv</a>
                <p></p>
                <p>
                Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x.
                </p>
              </td>
            </tr> -->

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="assets/qubit.png" alt="blind-date" width="300" height="150">
              </td>
              <td width="75%" valign="middle">
                <a href="https://github.com/freaksie/quantumReadOut">
                  <span class="papertitle">FPGA-Accelerated Machine Learning for Rapid Quantum Qubit Readout</span>
                </a>
                <!-- <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                <br>
                <em>The Astronomical Journal</em>, 136, 2008 -->
                <p>
                  This project swiftly classified quantum qubit states using ML algorithms. Raw voltage signals from a cryogenically 
                  stored quantum chip were processed, and noise removal and dimension reduction were performed. 
                  The ML algorithms, along with a data-driven digital local oscillator, enabled high-fidelity 
                  classification—all implemented on FPGAs, completing the process in just 2µs.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="assets/multimodal.png" alt="blind-date" width="300" height="150">
              </td>
              <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/pdf/10.1145/3597060.3597237">
                  <span class="papertitle">Multimodal UAV Detection and Tracking System</span>
                </a>
                <!-- <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                <br>
                <em>The Astronomical Journal</em>, 136, 2008 -->
                <p>
                  This project addresses limitations in camera-based tracking systems by implementing a multi-modal 
                  approach, combining both acoustic and vision tracking. Leveraging student-teacher learning, 
                  the acoustic model was trained from the pre-trained vision model. The integration of a cross-attention 
                  mechanism maximized the advantages of both modalities. The project demonstrates the efficacy of this
                  approach in UAV detection and tracking. Implemented on the ARM Cortex A72, the system ensures 
                  lightweight and real-time inference for practical applications.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="assets/signal_comp.png" alt="clean-usnob" width="300" height="150">
              </td>
              <td width="75%" valign="middle">
                <a href="#">
                  <span class="papertitle">EMG Signal Compression for Epileptic Seizure Detection with Variational Auto-Encoder</span>
                </a>
                <!-- <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://stumm.ca/">Christopher Stumm</a>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                <br>
                <em>The Astronomical Journal</em>, 135, 2008 -->
                <p>This project focuses on extracting valuable information from physiological signals and compressing 
                  their size. Utilizing a variational auto-encoder, traditionally employed for images, the project
                  compresses physiological signals. To showcase its practical application, the approach was implemented
                  on a real-life seizure patient. The compressed signals were then utilized for seizure detection as
                  a downstream task, validating the effectiveness of the compression method. The entire system was 
                  implemented on ARM Cortex A72 and Cortex A57, highlighting real-time capabilities.</p>
              </td>
            </tr>

          </tbody></table>

          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <hr>
              <h2>Experiences</h2>
            </td>
          </tr>
        </tbody></table>  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <a href="https://www.lbl.gov/">
                <img src="assets/lbnl.jpg" alt="blind-date" width="150" height="150"></a>
              </td>
              <td width="75%" valign="middle">
                <a href="https://www.lbl.gov/">
                  <span class="papertitle">Lawerence Berkeley National Laboratory</span>
                </a>
                <br>
                <strong><em>MLE Research Intern</em></strong>
                <table style="width: 100%;">
                  <tr>
                    <td style="width: 50%;">
                      <em>Berkeley, CA </em>
                    </td>
                    <td style="width: 50%; text-align: right;">
                      <em>(Sept 2023)</em>
                    </td>
                  </tr>
                </table>
                <p>At LBNL, I optimized superconducting quantum qubit readout, reducing time from 4μs to 1.5µs using
                   signal processing and noise reduction. Developed a Pytorch LSTM achieving 98% fidelity for discriminating 
                   8-qubit states. Applied agile practices with TensorFlow, Kubernetes, and Grafana. 
                   Translated the model to HDL and implemented it on an FPGA with model inference time of just 24ns.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle" align="center">
                <a href="https://wsslab.org/">
                  <img src="assets/wsslab.jpg" alt="clean-usnob" width="150" height="150" ></a>
              </td>
              <td width="75%" valign="middle">
                <a href="https://wsslab.org/">
                  <span class="papertitle">Wireless and Sensor System Lab</span>
                </a>
                <br>
                <strong><em>MLE Research Intern</em></strong>
                <table style="width: 100%;">
                  <tr>
                    <td style="width: 50%;">
                     <em>Amherst, MA </em>
                    </td>
                    <td style="width: 50%; text-align: right;">
                      <em>(May 2023 - Aug 2023)</em>
                    </td>
                  </tr>
                </table>
                <p>At WSSLab, I achieved a 1:293 compression ratio for EEG signals using VAE. 
                  Implemented a real-time data pipeline with ARM Cortex and Nvidia Jetson. 
                  Developed a scalable patient's seizure monitoring and detection system deployed GCP using Node.js and 
                  machine learning. Created interactive interfaces with React and established 
                  MongoDB backend. Validated the end-to-end system for EEG data through quantitative 
                  metrics and real-world applications</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle" align="center">
                <a href="https://uta.edu/">
                  <img src="assets/uta.jpeg" alt="clean-usnob" width="120" height="100" >
                </a>
                
              </td>
              <td width="75%" valign="middle">
                <a href="https://uta.edu/">
                  <span class="papertitle">University of Texas at Arlington</span>
                </a>
                <br>
                <strong><em>Graduate Research Assistant</em></strong>
                <table style="width: 100%;">
                  <tr>
                    <td style="width: 50%;">
                     <em>Arlington, TX </em>
                    </td>
                    <td style="width: 50%; text-align: right;">
                      <em>(Sept 2022 - Apr 2023)</em>
                    </td>
                  </tr>
                </table>
                <p>
                  <em><b>Domain of Object Tracking</b></em><br> UTA, I enhanced UAV detection by 26% using cross-modal self-supervised 
                  learning, surpassing state-of-the-art models in non-line-of-sight scenarios. 
                  Created a unique CRNN-based acoustic model and employed YOLO as a teacher model 
                  for UAV detection in low-light (1.75 Lux) and Blockage conditions.<br> 
                  <em><b>Domain of Genome</b></em><br>
                  Implemented GANs to reconstruct the g-carbon distance matrix and utilized ADMM 
                  for 3D protein structure conversion, enabling more accurate and efficient 
                  representation of molecular structures for advanced biomedical applications. 
                  <br> 
                  <em><b>Domain of NLP</b></em><br>
                  Improved LLM (llama with 13 billion parameters) inference throughput 
                  with a paged attention technique. Established a Kubernetes-based pipeline, reducing 
                  workflow latency by 15% through efficient data management. 
                  Collaborated with a cross-functional team to integrate the app with existing systems, 
                  boosting system efficiency by 10%.</p>
              </td>
            </tr>

            <!-- <tr>
              <td style="padding:20px;width:25%;vertical-align:middle" align="center">
                <img src="assets/ttmg.jpeg" alt="clean-usnob" width="120" height="100" >
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
                  <span class="papertitle">The Tann Mann Gaadi</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://stumm.ca/">Christopher Stumm</a>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                <br>
                <em>The Astronomical Journal</em>, 135, 2008
                <p>We use computer vision techniques to identify and remove diffraction spikes and reflection halos in the USNO-B Catalog.</p>
                <p>In use at <a href="http://www.astrometry.net">Astrometry.net</a></p>
              </td>
            </tr> -->
            
            
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <hr>
              <h2>Research</h2>
            </td>
            </tr>
          </tbody></table>  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='zipnerf_image'><video  width="290" height=150 muted autoplay loop>
                  <source src="assets/droneDemo.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <div id="drone_image"><img src='assets/dronechase.png' width="290" height="150"></div>
                  
                </div>
                <script type="text/javascript">
                  function zipnerf_start() {
                    document.getElementById('zipnerf_image').style.opacity = "1";
                    document.getElementById('drone_image').style.opacity = "0";
                  }
      
                  function zipnerf_stop() {
                    document.getElementById('zipnerf_image').style.opacity = "0";
                    document.getElementById('drone_image').style.opacity = "1";
                  }
                  zipnerf_stop()
                </script>
              </td>
              <!-- <td style="padding:20px;width:25%;vertical-align:middle", align="center">
                <img src="assets/dronechase.png" alt="blind-date" width="250" height="150">
              </td> -->
              <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/pdf/10.1145/3597060.3597237">
                  <span class="papertitle">DroneChase: A Mobile and Automated Cross-Modality System for Continuous Drone Tracking</span>
                </a>
                <br>
                <strong>Neel R Vora</strong>, <a href="https://web.eecs.utk.edu/~ywu83/">Yi Wu</a>, <a href="https://www.eecs.utk.edu/people/jian-liu/">Jian Liu</a>, <a href="https://people.cs.umass.edu/~phuc/">Phuc Nguyen</a>
                <br>
                <em>ACM, MOBISYS, DroNet'23</em>
                <p>The paper presents DroneChase, a mobile drone tracking system using a camera 
                  and microphone array that achieves robust performance under obscured conditions 
                  by fusing acoustic and visual modalities in a self-supervised, cross-modality 
                  framework</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="assets/vae.png" alt="clean-usnob" width="300" height="150" >
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2312.12587.pdf">
                  <span class="papertitle">Real-Time Diagnostic Integrity Meets Efficiency:
                    A Novel Platform-Agnostic Architecture for
                    Physiological Signal Compression</span>
                </a>
                <br>
                <strong>Neel R Vora</strong>, Amir Hajighasemi, Cody T. Reynolds, ET al.
                <br>
                <em>In Press</em>, 2024
                <p>This paper proposes a variational autoencoder architecture that compresses 
                  multi-channel physiological signals by over 293x while retaining 91% seizure 
                  detection accuracy. Validated on real EEG data from epilepsy patients, 
                  it demonstrates clinical utility and enables substantial power savings of up 
                  to 26.8% on edge devices.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle" align="center">
                <img src="assets/earsd.png" alt="clean-usnob" width="300" height="120" >
              </td>
              <td width="75%" valign="middle">
                <a href="#">
                  <span class="papertitle">Epileptic Seizure Detection and Classification Using Earable</span>
                </a>
                <br>
                Abdul Aziz, <a href="https://www.agilecps.org/nick">Nhat Pham</a>, <strong>Neel R Vora</strong>, ET al.
                <br>
                <em>In Press</em>, 2024
                <p>EarSD applies lightweight ML models including SVM, KNN, and Random 
                  Forests to detect epileptic seizures. The models are trained on processed 
                  EEG, EMG, and EOG signals acquired from behind the ears of 33 epilepsy 
                  patients. They achieve up to 95.3% seizure detection accuracy, on par 
                  with video-EEG.</p>
              </td>
            </tr>
            
            
          </tbody></table>
          <hr>
          <section style="display: flex; justify-content: space-between; margin-bottom: 20px;">
            <div>
              <h2>Interest</h2>
              <ul>
                <li><strong>Computer Vision</strong></li>
                <li><strong>NLP</strong></li>
                <li><strong>Deep Reinforcement Learning</strong></li>
                <li><strong>ML on Edge</strong></li>
              </ul>
              <br><br><br>
              <h2>Services</h2>
              <ul>
                <li>
                  Visiting scholar at <a href="https://www.umass.edu/">University of Massachusetts Amherst.</a>
                </li>
                <li>
                  Reviewer at The 30th Annual International Conference On
                  Mobile Computing And Networking <a href="https://www.sigmobile.org/mobicom/2024/">(MobiCom '24)</a>
                </li>
                <li>Former senior ML team member at <a href="https://gdsc.community.dev/dharmsinh-desai-university-nadiad/">GDSC-DDU</a></li>
              </ul>
            </div>
        
            <div>
              <h2>Education</h2>
                <strong>The University of Texas at Arlington</strong>
                  <br>Degree: Masters of Science in Computer Science
                  <br>Graduation Year: 2024
                <br><br>
                <strong>DHARMSINH DESAI UNIVERSITY</strong>
                  <br>Degree: Bachelor of Technology in Information Technology
                  <br>Graduation Year: 2022
            </div>
          </section>

        

          


          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Thanks <a href="https://jonbarron.info/">Jon Barron</a>, for this website template.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>