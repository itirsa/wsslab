<!DOCTYPE html>
<html class=" js flexbox flexboxlegacy canvas canvastext webgl no-touch
  geolocation postmessage no-websqldatabase indexeddb hashchange history
  draganddrop websockets rgba hsla multiplebgs backgroundsize
  borderimage borderradius boxshadow textshadow opacity cssanimations
  csscolumns cssgradients no-cssreflections csstransforms
  csstransforms3d csstransitions fontface generatedcontent video audio
  localstorage sessionstorage webworkers applicationcache svg inlinesvg
  smil svgclippaths" style="">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="description" content="Earable Tutorial at MobiCom 2022">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Earable Tutorial at MobiCom 2022 | Welcome</title>
    <link rel="apple-touch-icon" href="./apple-touch-icon.png">
    <link rel="shortcut icon" type="image/x-icon" href="./favicon.ico">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Earable Tutorial at MobiCom 2022">
    <meta property="og:url"
      content="https://www.sigmobile.org/mobisys/2020/">
    <meta property="og:site_name" content="Earable Tutorial at MobiCom 2022">
    <meta property="og:description" content="Earable Tutorial at MobiCom 2022.">
    <meta property="og:image"
content="Tutorial_files/banner.jpg">
    <link rel="stylesheet" href="Tutorial_files/main.css">
    <link href="Tutorial_files/css.css" rel="stylesheet"
      type="text/css">
    <!-- <script src="Tutorial_files/sdk_002.js" async="" crossorigin="anonymous"></script> -->
    <script id="facebook-jssdk" src="Tutorial_files/sdk.js"></script>
    <script id="twitter-wjs" src="Tutorial_files/widgets.js"></script>
    <script async="" src="Tutorial_files/analytics.js"></script>
    <script src="Tutorial_files/modernizr.js"></script>
    <style type="text/css">.fb_hidden{position:absolute;top:-10000px;z-index:10001}.fb_reposition{overflow:hidden;position:relative}.fb_invisible{display:none}.fb_reset{background:none;border:0;border-spacing:0;color:#000;cursor:auto;direction:ltr;font-family:"lucida grande", tahoma, verdana, arial, sans-serif;font-size:11px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal}.fb_reset>div{overflow:hidden}@keyframes fb_transform{from{opacity:0;transform:scale(.95)}to{opacity:1;transform:scale(1)}}.fb_animate{animation:fb_transform .3s forwards}
.fb_dialog{background:rgba(82, 82, 82, .7);position:absolute;top:-10000px;z-index:10001}.fb_dialog_advanced{border-radius:8px;padding:10px}.fb_dialog_content{background:#fff;color:#373737}.fb_dialog_close_icon{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 0 transparent;cursor:pointer;display:block;height:15px;position:absolute;right:18px;top:17px;width:15px}.fb_dialog_mobile .fb_dialog_close_icon{left:5px;right:auto;top:5px}.fb_dialog_padding{background-color:transparent;position:absolute;width:1px;z-index:-1}.fb_dialog_close_icon:hover{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -15px transparent}.fb_dialog_close_icon:active{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -30px transparent}.fb_dialog_iframe{line-height:0}.fb_dialog_content .dialog_title{background:#6d84b4;border:1px solid #365899;color:#fff;font-size:14px;font-weight:bold;margin:0}.fb_dialog_content .dialog_title>span{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yd/r/Cou7n-nqK52.gif) no-repeat 5px 50%;float:left;padding:5px 0 7px 26px}body.fb_hidden{height:100%;left:0;margin:0;overflow:visible;position:absolute;top:-10000px;transform:none;width:100%}.fb_dialog.fb_dialog_mobile.loading{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/ya/r/3rhSv5V8j3o.gif) white no-repeat 50% 50%;min-height:100%;min-width:100%;overflow:hidden;position:absolute;top:0;z-index:10001}.fb_dialog.fb_dialog_mobile.loading.centered{background:none;height:auto;min-height:initial;min-width:initial;width:auto}.fb_dialog.fb_dialog_mobile.loading.centered #fb_dialog_loader_spinner{width:100%}.fb_dialog.fb_dialog_mobile.loading.centered .fb_dialog_content{background:none}.loading.centered #fb_dialog_loader_close{clear:both;color:#fff;display:block;font-size:18px;padding-top:20px}#fb-root #fb_dialog_ipad_overlay{background:rgba(0, 0, 0, .4);bottom:0;left:0;min-height:100%;position:absolute;right:0;top:0;width:100%;z-index:10000}#fb-root #fb_dialog_ipad_overlay.hidden{display:none}.fb_dialog.fb_dialog_mobile.loading iframe{visibility:hidden}.fb_dialog_mobile .fb_dialog_iframe{position:sticky;top:0}.fb_dialog_content .dialog_header{background:linear-gradient(from(#738aba), to(#2c4987));border-bottom:1px solid;border-color:#043b87;box-shadow:white 0 1px 1px -1px inset;color:#fff;font:bold 14px Helvetica, sans-serif;text-overflow:ellipsis;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0;vertical-align:middle;white-space:nowrap}.fb_dialog_content .dialog_header table{height:43px;width:100%}.fb_dialog_content .dialog_header td.header_left{font-size:12px;padding-left:5px;vertical-align:middle;width:60px}.fb_dialog_content .dialog_header td.header_right{font-size:12px;padding-right:5px;vertical-align:middle;width:60px}.fb_dialog_content .touchable_button{background:linear-gradient(from(#4267B2), to(#2a4887));background-clip:padding-box;border:1px solid #29487d;border-radius:3px;display:inline-block;line-height:18px;margin-top:3px;max-width:85px;padding:4px 12px;position:relative}.fb_dialog_content .dialog_header .touchable_button input{background:none;border:none;color:#fff;font:bold 12px Helvetica, sans-serif;margin:2px -12px;padding:2px 6px 3px 6px;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}.fb_dialog_content .dialog_header .header_center{color:#fff;font-size:16px;font-weight:bold;line-height:18px;text-align:center;vertical-align:middle}.fb_dialog_content .dialog_content{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/y9/r/jKEcVPZFk-2.gif) no-repeat 50% 50%;border:1px solid #4a4a4a;border-bottom:0;border-top:0;height:150px}.fb_dialog_content .dialog_footer{background:#f5f6f7;border:1px solid #4a4a4a;border-top-color:#ccc;height:40px}#fb_dialog_loader_close{float:left}.fb_dialog.fb_dialog_mobile .fb_dialog_close_button{text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}.fb_dialog.fb_dialog_mobile .fb_dialog_close_icon{visibility:hidden}#fb_dialog_loader_spinner{animation:rotateSpinner 1.2s linear infinite;background-color:transparent;background-image:url(https://static.xx.fbcdn.net/rsrc.php/v3/yD/r/t-wz8gw1xG1.png);background-position:50% 50%;background-repeat:no-repeat;height:24px;width:24px}@keyframes rotateSpinner{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}
.fb_iframe_widget{display:inline-block;position:relative}.fb_iframe_widget span{display:inline-block;position:relative;text-align:justify}.fb_iframe_widget iframe{position:absolute}.fb_iframe_widget_fluid_desktop,.fb_iframe_widget_fluid_desktop span,.fb_iframe_widget_fluid_desktop iframe{max-width:100%}.fb_iframe_widget_fluid_desktop iframe{min-width:220px;position:relative}.fb_iframe_widget_lift{z-index:1}.fb_iframe_widget_fluid{display:inline}.fb_iframe_widget_fluid span{width:100%}</style>
  </head>
  <body>
    <!--[if lt IE 10]>
 <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
 <![endif]-->
    <div id="wrapper"> <button type="button" class="visible-xs
        hamburger is-closed"> <span class="hamb-top"></span> <span
          class="hamb-middle"></span> <span class="hamb-bottom"></span>
      </button>
      <div class="container" id="page-content-wrapper">
        <div class="row visible-xs" id="mobile-spacer"> <span
            class="glyphicon glyphicon-menu-left"></span> Navigation </div>
        <div class="row">
          <header id="banner" class="col-xs-12 col-sm-12 col-lg-12"> <img
              class="img-rounded img-responsive" alt="ACM MobiSys 2018
              Banner" src="Sydney_night.jpeg"> </header>
        </div>
        <div class="row">
          <aside id="nav" class="col-sm-3 col-lg-3">
            <nav>
              <div class="panel-group" id="accordion" role="tablist"
                aria-multiselectable="true">
                <div class="panel panel-default">
                  <div class="panel-heading" role="tab" id="headingOne"
                    onclick="window.location =
                    'https://www.sigmobile.org/mobicom/2022/'">
                    <h4 class="panel-title">MobiCom 2022</h4>
                  </div>
                </div>
                <div class="panel panel-default">
                  <div class="panel-heading" role="tab" id="headingOne"
                    onclick="window.location = './index.html'">
                    <h4 class="panel-title">Main</h4>
                  </div>
                </div>
                <div class="panel panel-default">
                  <div class="panel-heading" role="tab" id="headingOne"
                    onclick="window.location = 'https://www.sigmobile.org/mobicom/2022/registration.html'">
                    <h4 class="panel-title">Registration</h4>
                  </div>
                </div>
                <div class="panel panel-default">
                  <div class="panel-heading" role="tab" id="headingOne"
                    onclick="window.location = './'">
                    <h4 class="panel-title">Schedule</h4>
                  </div>
                </div>
                <!-- <div class="panel panel-default">
                  <div class="panel-heading" role="tab" id="headingOne"
                    onclick="window.location = './program_final/'">
                    <h4 class="panel-title">Program</h4>
                  </div>
                </div> -->
                <!-- <div class="panel panel-default">
                  <div class="panel-heading" role="tab" id="headingOne"
                    onclick="window.location = './cfp/'">
                    <h4 class="panel-title">Call For Papers</h4>
                  </div>
                </div>
                <div class="panel panel-default">
                  <div class="panel-heading" role="tab" id="headingOne"
                    onclick="window.location = './submission/'">
                    <h4 class="panel-title">Paper Submissions</h4>
                  </div>
                </div> -->
                <!-- <div class="panel panel-default">
                  <div class="panel-heading" role="tab" id="headingOne"
                    onclick="window.location = './camera_ready/'">
                    <h4 class="panel-title">Camera-Ready Instructions</h4>
                  </div>
                </div> -->
              </div>
            </nav>
          </aside>
          <div class="col-sm-9 col-lg-9">

            <h1>Technical Program</h1>


              <h4>October 17, 2022  (Australian Eastern Daylight Time, AEDT)</h4>
              <!-- <h3 style="color:red">Join Zoom Meeting: <a href="https://cuboulder.zoom.us/j/3521084554">https://cuboulder.zoom.us/j/3521084554</a></h3> -->
              <h3 style="color:red">Join Slack Channel: <a href="https://tinyurl.com/4bz9yvkh">https://tinyurl.com/4bz9yvkh</a></h3>

              <h4>------------//------------</h4>
              <h3>Welcome from Chairs  <small>8:00&ndash;8:15</small></h3>

              <h3>Lecture 1: "Wear-to-compute? Challenges of earable computing for health" <small>8:15&ndash;9:00</small></h3>
              <p align="left"><img style="border:0px" src="img/prof.cecilia.jpeg" width="300"></p>
              <h4>Speaker: <a href="https://www.cl.cam.ac.uk/users/cm542">Prof. Cecilia Mascolo</a> (University of Cambridge, UK)</h4>
              <h4>Abstract</h4>
              <p>Earable devices are pervasive in our lives, they accompanying us in every virtual meeting or while we are running. These devices are becoming, in theory, very good proxies for human behaviour. Yet, making the inference from the raw sensor data to individuals’ behaviour remains difficult.

              In this talk I will discuss and highlight the open challenges that these technologies still face before they can be trusted health measurement proxies. Namely, the ability to work in the wild, the sensitivity of the data versus centralisation of computation, the uncertainty of the prediction over the data. I will use examples from my group's ongoing research on on-device machine learning, earable sensing and uncertainty estimation for health application in collaboration with epidemiologists and clinicians.
              </p>
              <h4>Speaker Bio</h4>
              <!-- <p align="left"><img style="border:0px" src="img/chowdhury-k.jpg" width="200"></p> -->
               <p>
              Cecilia Mascolo is the mother of a teenage daughter but also a Full Professor of Mobile Systems in the Department of Computer Science and Technology, University of Cambridge, UK. She is director of the <a href="https://mobicentre.cst.cam.ac.uk/">Centre for Mobile, Wearable System and Augmented Intelligence</a>. She is also a Fellow of Jesus College Cambridge and the recipient of an ERC Advanced Research Grant. Prior joining Cambridge in 2008, she was a faculty member in the Department of Computer Science at University College London. She holds a PhD from the University of Bologna. Her research interests are in mobile systems and machine learning for mobile health. She has published in a number of top tier conferences and journals in the area and her investigator experience spans projects funded by Research Councils and industry. She has served as steering, organizing and programme committee member of mobile and sensor systems, data science and machine learning conferences. More details at <a href="https://www.cl.cam.ac.uk/users/cm542"> www.cl.cam.ac.uk/users/cm542</a>.
               </p>

              <h3>Lecture 2: "Sensing and Stimulation with Earable Devices " <small>9:00&ndash;10:00</small></h3>
              <p align="left"><img style="border:0px" src="img/prof-TamVu-v7.png" width="300"></p>
              <h4>Speaker: <a href="http://mnslab.org/tamvu/">Prof. Tam Vu</a> (CEO, Earable Inc. and University of Colorado Boulder, USA)</h4>
              <h4>Abstract</h4>
              <p>
              This talk discusses Earable computers as sensing and actuating devices that are worn inside, behind, around, or on users' ears for sensing many important physiological signals such as the brain, eyes, facial muscles, heart rate, blood pressure, core body temperature, and more. These signals could enable a wide range of applications from human-computer interaction, to health care, attention/focus monitoring, and opioid use reduction. Drawing the analogy from the evolutions of mobile systems and wearable systems, in this talk, I will discuss the opportunities that earable systems could bring. I will share our experience and lessons learned through realizing such earable systems in the healthcare context, from research to commercialization.

              </p>
              <h4>Speaker Bio</h4>
              <!-- <p align="left"><img style="border:0px" src="img/chowdhury-k.jpg" width="200"></p> -->
               <p>
                 Tam Vu is the CEO of Earable Inc., a neuroscience company developing wearables that detect real-time neuro signals to stimulate the brain to improve sleep, focus, and other cognitive functions; and is on-leave from the University of Colorado Boulder. He was an associate professor at Oxford University in the UK before he left to focus more on the company. He leads the Mobile and Networked Systems(MNS) Lab where he and his team conduct system research in the areas of wearable and mobile systems, exploring the physiological signals of a user and using them for inventing new human-computer interaction techniques and health-care solutions. Together with his teams, the outcomes of his works resulted in a CES Innovation Award 2023, MUSE Design Award, VMark Design Award, Sloan Fellowship, NSF CAREER award, two Google Faculty Awards, 12 best paper awards, best paper nominations, and research highlights in flagship venues in mobile system research. He is also actively pushing his research outcomes to practice through technology transfer activities with 35 patents filed and attracted external investment for 2 venture-backed companies that he co-founded to commercialize them. One of the companies that he found, named Earable Inc, has raised $10M from leading investors to launch its product to the market later this year to help the world sleep better. 
               </p>

              <h3>Coffee Break  <small>10:00&ndash;10:15</small></h3>

              <h3>Lecture 3: "Leveraging Earables for Unvoiced Speech and Beyond" <small>10:15&ndash;11:00</small></h3>
              <p align="left"><img style="border:0px" src="img/shubham_jain.JPG" width="300"></p>
              <h4>Speaker: <a href="https://www3.cs.stonybrook.edu/~jain/">Prof. Shubham Jain</a> (Stony Brook University, USA)</h4>
              <h4>Abstract</h4>
              <p>
              Modern environments are alive with sensors, such as smartphones, watches, earphones and other wearables. However, despite the plethora of sensors, technologies supporting the disabled population are still scarce. This is primarily due to the difficulty in direct sensing of desired events in the real-world. We believe that earables are the next frontier in passive sensing that offer a novel interaction modality and can provide a range of services to a diverse population. This talk will explore our research efforts in leveraging the inertial sensors on earables to interpret unvoiced or silent speech. Not only is this a privacy preserving interaction modality, but it holds great promise for those with speech disabilities.

              </p>
              <h4>Speaker Bio</h4>
              <!-- <p align="left"><img style="border:0px" src="img/chowdhury-k.jpg" width="200"></p> -->
               <p>
              Shubham Jain is an Assistant Professor in the Department of Computer Science at Stony Brook University where she leads <a href="http://picasso.cs.stonybrook.edu/">PiCASSo</a> (Pervasive Computing and Smart Sensing) Lab. Her research interests lie in cyber-physical systems, mobile health, and data analytics for wearable sensing. Her work on pedestrian safety has been featured in several media outlets, including The Wall Street Journal. She received her PhD in Electrical & Computer Engineering from Rutgers University in 2017.
               </p>


               <h3>Lecture 4: "Revisiting a Few Classical Problems in the Context of Earable Computing" <small>11:00&ndash;12:30</small></h3>
              <p align="left"><img style="border:0px" src="img/romit.jpg" width="300"> <img style="border:0px" src="img/wally.jpg" width="289"></p>
              <h4>Speaker: <a href="https://croy.web.engr.illinois.edu/">Prof. Romit Roy</a> and <a href="https://yulinlw2.web.illinois.edu/">Yu-Lin Wei</a> (University of Illinois Urbana-Champaign, USA)</h4>
              <h4>Abstract</h4>
              <p>
              The rapid advances in speech recognition, NLP, and human-centric sensing make the earable platform a launching pad for many new technologies and applications. On the other hand, earables also encapsulate a much higher information density, complexity, and constraints, compared to other mobile devices. In this talk, we will revisit classical problems under the new challenges brought by the earable computing platform. For instance, how could we estimate multiple angle of arrivals (AoAs) in a heavily reverberated environment? How could we separate multiple ambient signals with a few microphones? How could we locate and calibrate a user’s location without help from other infrastructure? This talk will take quick glimpses on classical approaches in each of these topics, and then discuss some new ideas (both algorithmic and deep-learning-based) for the special case of earable computing.

              </p>
              <h4>Speaker Bio</h4>
              <!-- <p align="left"><img style="border:0px" src="img/chowdhury-k.jpg" width="200"></p> -->
               <p>
                 Yu-Lin Wei (Wally) is a 5th year Ph.D. student from the Electrical Computer Engineering department at UIUC. He got his bachelor's and master's degrees from Computer Science and Information Engineering department at National Taiwan University. He received the ECE Rambus fellowship and was selected as the young researcher at Heidelberg Laureate Forum. His research interests include acoustic signal processing, earable computing, wireless and visible communication, and indoor positioning.</br>
                 </br>

                 Romit Roy Choudhury is a Jerry Sanders III AMD Scholar and Professor of ECE and CS at the University of Illinois at Urbana Champaign (UIUC). He joined UIUC from Fall 2013, prior to which he was an Associate Professor at Duke University. Romit received his PhD in the CS department of UIUC in Fall 2006. His research interests are in applied signal processing, with a focus on audio sensing and inferencing. For more information, visit Romit's Systems Networking Research Group (SyNRG) at <a href="http://synrg.csl.illinois.edu">http://synrg.csl.illinois.edu</a>
               </p>

              <h3>Lunch Break  <small>12:30&ndash;1:15</small></h3>

              <h3>Lecture 5: "Multimodal and Transfer Learning for Fine-Grained Human Facial Sensing Using Earables" <small>1:15&ndash;02:00</small></h3>
              <p align="left"><img style="border:0px" src="img/vpnguyen.jpeg" width="300"></p>
              <h4>Speaker: <a href="http://wsslab.org/vpnguyen/">Prof. VP Nguyen</a> (University of Texas at Arlington, USA)</h4>
              <h4>Abstract</h4>
              <p>
              In this talk, I will discuss an important emerging research topic of developing a socially-acceptable earable system that can unobtrusively, continuously, and reliably sense human facial activities in fine-grained. Building such a system requires answering many important questions. For example, where do we place sensors or electrodes? How many sensing modalities, electrodes, or channels are sufficient? How do we select the best combination of sensors, electrodes, and channels for the desired applications? How can we adaptively optimize the contributions of each sensing input? Can we allow sensing modalities to learn from each other to enhance accuracy? Can the knowledge be transferred from high-dimensional data to one or few dimensional data? This talk will describe the roles of multimodal and transfer learning in addressing these important questions. I will then discuss our experiences and lessons learned in developing novel algorithms to allow earables to reliably track 3D facial activities and face-hand interaction. I will close the talk by presenting other opportunities that the proposed techniques could bring to earable research and challenges that need to be addressed to unleash their potential.

              </p>
              <h4>Speaker Bio</h4>
              <!-- <p align="left"><img style="border:0px" src="img/chowdhury-k.jpg" width="200"></p> -->
               <p>
               VP Nguyen is an Assistant Professor of Computer Science and Engineering at the University of Texas at Arlington, where he directs the <a href="http://wsslab.org/">Wireless and Sensor Systems Laboratory (WSSL)</a>. His lab focuses on building low-power, battery-free, and intelligent cyber-physical systems for smart-health, precision agriculture, 5G/6G communication, and UAV security and privacy. He is the recipient of SONY Faculty Innovation Award 2021, University of Texas System Rising Stars Award, CACM Research Highlights 2021, ACM SIGMOBILE Research Highlights 2017, 2020, 2022, Best Paper Award at ACM MobiCom 2019, Best Poster Award at IPSN 2022, Best Paper Runner up Award at ACM SenSys 2018, Best Paper Nominee at ACM SenSys 2017, Best Paper Awards at ACM MobiCom-S3 2016-2017. He obtained his Ph.D. degree in Computer Science from the University of Colorado Boulder.
               </p>

              <h3>Lecture 6: Robust Sensing and Efficient Computing for Intelligent Biosignal-based Earables. <small>02:00&ndash;02:45</small></h3>
              <p align="left"><img style="border:0px" src="img/nhat_ox.jpeg" width="300"></p>
              <h4>Speaker: <a href="https://www.nhatpham.info/">Nhat Pham</a> (University of Oxford, UK)</h4>
              <h4>Abstract</h4>
              <p>
              Earables have great potential in enabling various healthcare applications due to their close proximity to multiple sources of critical head-based biosignals such as EEG, EOG, EMG, and EDA originating from the brain, eyes, facial muscles, and sweat glands, respectively. However, there are several fundamental challenges to realize their potential, such as (1) how we can capture biosignals reliably given the noisy environments and (2) how we can optimize the system to ensure high-fidelity signals while being energy-efficient for prolonged usage. This talk will discuss our recent research efforts in hardware and software techniques to overcome these challenges. We will also take a glimpse on the potential use cases such as microsleep detection or long-term epileptic seizure monitoring.

              </p>
              <h4>Speaker Bio</h4>
              <!-- <p align="left"><img style="border:0px" src="img/chowdhury-k.jpg" width="200"></p> -->
               <p>
              Nhat (Nick) Pham is a final-year Ph.D. candidate at the Department of Computer Science, University of Oxford. His Ph.D. study is supervised by Prof. Niki Trigoni, Prof. Andrew Markam, and Prof. Tam Vu and is funded by the University of Oxford Scholarship. He received his M.Sc. in Computer Science and B.Eng. in Computer Engineering from the Korea Advanced Institute of Science and Technology (KAIST) in 2018 and Vietnam National University - Ho Chi Minh City University of Technology (HCMUT) in 2015, respectively. He is the recipient of CACM Research Highlight 2021, ACM SIGMOBILE Research Highlight 2020, ACM GetMobile Research Highlight 2019, the Best Paper Award at ACM MobiCom 2019, KAIST Graduate Scholarship, HCMUT Silver Graduation Medal, and Odon Vallet's Scholarship. His current research interests include Intelligent Wearables for Healthcare and well-being, Edge-AI and On-chip Computing, and Quantum-based Human Sensing.
               </p>

              <h3>Lecture 7: "Programming Earable BrainBand Made Easy" <small>02:45&ndash;04:30</small></h3>
              <p align="left"><img style="border:0px" src="img/galen.jpeg" width="300"> <img style="border:0px" src="img/vinh.jpg" width="220"></p>
              <h4>Speaker: Galen Pogoncheff (galen@earable.ai) and Vinh Nguyen (vinhnx@earable.ai)</h4>
              <h4>Abstract</h4>
              <p>
              This talk presents Earable BrainBand - Frenz - from a researcher's perspective and a live-coding demo. We present technical details of Earable Brainband, the data that we receive, how it can be used in research, the current set of available features, etc.  We then do a live implementation of 3 simple demos that highlight relevant ideas for researchers.  In this live coding session and demo, we will show a basic setup for connecting to a band with a bluetooth connection, starting data streaming, preprocessing the raw signals, and visualizing the signals in real-time.  Since a lot of prospective researchers desire the ability to collect signal data directly from the device, this will be a very important feature of our SDK to show. We will then expand upon this code, demonstrating facial gesture detection and focus.  Each of these features should be associated with an associated visual that keeps the audience engaged.

              </p>
              <h4>Speaker Bio</h4>
              <!-- <p align="left"><img style="border:0px" src="img/chowdhury-k.jpg" width="200"></p> -->
               <p>
                Galen Pogoncheff and Vinh Nguyen are Data scientists in <a href="https://earable.ai/">Earable Inc.</a>
              </p>

          </div>
        </div>
        <div class="row">
          <div class="footer">
            <div class="col-sm-offset-3 col-md-8 col-md-offset-3">
              <p>© Copyright 2022 WSSL, based on a template
                by Bastian Bloessl</p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <span id="top-link-block" class="affix-top"> <a href="#top"
        class="well well-sm"
        onclick="$('html,body').animate({scrollTop:0},'slow');return
        false;"> Back to Top </a> </span>
    <script src="Tutorial_files/vendor.js"></script>
    <script src="Tutorial_files/plugins.js"></script>
    <script src="Tutorial_files/main.js"></script> <iframe
      scrolling="no" allowtransparency="true"
      src="Tutorial_files/widget_iframe.html" title="Twitter settings
      iframe" style="display: none;" frameborder="0"></iframe>
    <div id="fb-root" class=" fb_reset">
      <div style="position: absolute; top: -10000px; width: 0px; height:
        0px;"> </div>
    </div>
  </body>
</html>
