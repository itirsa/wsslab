<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Wireless and Sensor Systems Lab (WSSL)</title>
    <link rel="shortcut icon" href="assets/images/favicon/favicon.ico" type="image/x-icon" />
    <meta name="keywords" content="HTML5,CSS3,Template" />
    <meta name="description" content="" />
    <meta name="Author" content="Dorin Grigoras [www.stepofweb.com]" />
    <style>
      .anchor {
        padding-top: 90px;
      }
    </style>
    <!-- mobile settings -->
    <meta name="viewport" content="width=device-width, maximum-scale=1, initial-scale=1, user-scalable=0" />
    <!-- WEB FONTS -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700,800" rel="stylesheet" type="text/css" />
    <!-- CORE CSS -->
    <link href="assets/plugins/bootstrap/css/bootstrap.min.css" rel="stylesheet" type="text/css" />
    <link href="assets/css/font-awesome.css" rel="stylesheet" type="text/css" />
    <link href="assets/plugins/owl-carousel/owl.carousel.css" rel="stylesheet" type="text/css" />
    <link href="assets/plugins/owl-carousel/owl.theme.css" rel="stylesheet" type="text/css" />
    <link href="assets/plugins/owl-carousel/owl.transitions.css" rel="stylesheet" type="text/css" />
    <link href="assets/plugins/magnific-popup/magnific-popup.css" rel="stylesheet" type="text/css" />
    <link href="assets/css/animate.css" rel="stylesheet" type="text/css" />
    <link href="assets/css/superslides.css" rel="stylesheet" type="text/css" />
    <!-- REVOLUTION SLIDER -->
    <link href="assets/plugins/slider.revolution.v4/css/settings.css" rel="stylesheet" type="text/css" />
    <!-- THEME CSS -->
    <link href="assets/css/essentials.css" rel="stylesheet" type="text/css" />
    <link href="assets/css/layout.css" rel="stylesheet" type="text/css" />
    <link href="assets/css/layout-responsive.css" rel="stylesheet" type="text/css" />
    <link href="assets/css/color_scheme/orange.css" rel="stylesheet" type="text/css" />

    <link href="assets/css/color_scheme/orange.css" rel="alternate stylesheet" type="text/css" title="orange" />
    <link href="assets/css/color_scheme/red.css" rel="alternate stylesheet" type="text/css" title="red" />
    <link href="assets/css/color_scheme/pink.css" rel="alternate stylesheet" type="text/css" title="pink" />
    <link href="assets/css/color_scheme/yellow.css" rel="alternate stylesheet" type="text/css" title="yellow" />
    <link href="assets/css/color_scheme/darkgreen.css" rel="alternate stylesheet" type="text/css" title="darkgreen" />
    <link href="assets/css/color_scheme/green.css" rel="alternate stylesheet" type="text/css" title="green" />
    <link href="assets/css/color_scheme/darkblue.css" rel="alternate stylesheet" type="text/css" title="darkblue" />
    <link href="assets/css/color_scheme/blue.css" rel="alternate stylesheet" type="text/css" title="blue" />
    <link href="assets/css/color_scheme/brown.css" rel="alternate stylesheet" type="text/css" title="brown" />
    <link href="assets/css/color_scheme/lightgrey.css" rel="alternate stylesheet" type="text/css" title="lightgrey" />
    <link href="assets/plugins/styleswitcher/styleswitcher.css" rel="stylesheet" type="text/css" />
    <!-- Morenizr -->
    <script type="text/javascript" src="assets/plugins/modernizr.min.js"></script>
    <style>
      .card {
        /* Add shadows to create the "card" effect */
        box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2);
        transition: 0.3s;
      }

      /* On mouse-over, add a deeper shadow */
      .card:hover {
        box-shadow: 0 8px 16px 0 rgba(0, 0, 0, 0.2);
      }

      button {
        border: none;
        outline: 0;
        display: inline-block;
        padding: 8px;
        color: white;
        background-color: #000;
        text-align: center;
        cursor: pointer;
        width: 100%;
        font-size: 18px;
      }
    </style>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169003278-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];

      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "UA-169003278-1");
    </script>
  </head>

  <body class="boxed pattern2">
    <header id="topNav">
      <div class="container">
        <button class="btn btn-mobile" data-toggle="collapse" data-target=".nav-main-collapse">
          <i class="fa fa-bars"></i>
        </button>

        <div class="navbar-collapse nav-main-collapse collapse pull-left">
          <h2><b>Wireless and Sensor Systems</b> Lab (WSSL)</h2>
        </div>
        <div class="navbar-collapse nav-main-collapse collapse pull-right">
          <nav class="nav-main mega-menu">
            <ul class="nav nav-pills nav-main scroll-menu" id="topMain">
              <li class="dropdown active">
                <a class="dropdown-toggle" href="index.html">
                  <b> Home</b>
                </a>
              </li>
              <li class="dropdown mega-menu-item mega-menu-two-columns">
                <a class="dropdown-toggle" href="#about">
                  <b>News</b>
                </a>
              </li>
              <li class="dropdown mega-menu-item mega-menu-two-columns">
                <a class="dropdown-toggle" href="index.html#projects">
                  <b>Projects</b>
                </a>
              </li>
              <li class="dropdown">
                <a class="dropdown-toggle" href="publications.html">
                  <b>Publications</b>
                </a>
              </li>
              <li class="dropdown">
                <a class="dropdown-toggle" href="members.html#members">
                  <b>Team</b>
                </a>
              </li>
              <li class="dropdown">
                <a class="dropdown-toggle" href="members.html#joinus">
                  <b>Join Us!</b>
                </a>
              </li>
            </ul>
          </nav>
        </div>
      </div>
    </header>

    <span id="header_shadow"></span>
    <!-- /TOP NAV -->
    <div id="wrapper">
      <!-- REVOLUTION SLIDER -->
      <div class="fullwidthbanner-container roundedcorners">
        <div class="fullwidthbanner">
          <ul>
            <li data-transition="fade" data-slotamount="14" data-masterspeed="300">
              <img
                src="assets/images/WSSL_Group_03.jpg"
                alt="church3"
                data-lazyload="assets/images/WSSL_2024.jpeg"
                data-fullwidthcentering="on"
              />

              <div
                class="tp-caption lfb modern_big_redbg"
                data-x="90"
                data-y="600"
                data-speed="300"
                data-start="800"
                data-easing="easeOutExpo"
              >
                WSSL (pronounced "Whistle") is a research lab at UMass Amhest exploring novel wireless, <br />
                mobile and wearable systems for healthcare and environmental monitoring.
              </div>
            </li>
            <li data-transition="fade" data-slotamount="14" data-masterspeed="300">
              <img
                src="assets/images/WSSL_Group_03.jpg"
                alt="church3"
                data-lazyload="assets/images/WSSL_2024_2.jpg"
                data-fullwidthcentering="on"
              />

              <div
                class="tp-caption lfb modern_big_redbg"
                data-x="90"
                data-y="600"
                data-speed="300"
                data-start="800"
                data-easing="easeOutExpo"
              >
                WSSL (pronounced "Whistle") is a research lab at the UMass Amhest exploring novel wireless,
                <br />
                mobile and wearable systems for healthcare and environmental monitoring.
              </div>
            </li>
            <li data-transition="fade" data-slotamount="14" data-masterspeed="300">
              <img
                src="assets/images/WSSL_Group_03.jpg"
                alt="church3"
                data-lazyload="assets/images/WSSL_Group_03.jpg"
                data-fullwidthcentering="on"
              />
              <div
                class="tp-caption lfb modern_big_redbg"
                data-x="90"
                data-y="600"
                data-speed="300"
                data-start="800"
                data-easing="easeOutExpo"
              >
                WSSL (pronounced "Whistle") is a research lab at the UMass Amhest exploring novel wireless,
                <br />
                mobile and wearable systems for healthcare and environmental monitoring.
              </div>
            </li>
            <li data-transition="fade" data-slotamount="14" data-masterspeed="300">
              <img
                src="assets/images/WSSL_Group_00.png"
                alt="church3"
                data-lazyload="assets/images/WSSL_Group_00.png"
                data-fullwidthcentering="on"
              />
              <div
                class="tp-caption lfb modern_big_redbg"
                data-x="90"
                data-y="600"
                data-speed="300"
                data-start="800"
                data-easing="easeOutExpo"
              >
                WSSL (pronounced "Whistle") is a research lab at the University of Texas at Arlington exploring novel wireless, <br />
                mobile and wearable systems for healthcare and environmental monitoring.
              </div>
            </li>
          </ul>
          <div class="tp-bannertimer"></div>
        </div>
      </div>

      <hr class="half-margins" />
      <center>
        <h4>Our research interests and topics include:</h4>
        <h4>
          <b>Internet of Medical Things</b>
        </h4>
        <h4>
          <b>Internet of Living Things</b>
        </h4>
        <h4>
          <b>Internet of Flying Things</b>
        </h4>
        <h4>
          <b>Quantum Computing</b>
        </h4>
      </center>

      <!-- ONE NATION -->
      <section class="container anchor" id="about">
        <div class="row">
          <div class="col-md-3">
            <h2>Lab <b>News</b></h2>
          </div>
          <div class="col-md-9">
            <!-- <h2 style="text-align: left;">Lab <b>News</b></h2> -->
            <ul style="list-style-type: disc">
              <li>
                09 - 2024: Unvoiced paper is accepted to
                <a target="_blank" href="https://sensys.acm.org/2024/" class="highlight">ACM Sensys 2024</a>
                (Acceptance Ratio ~18.5%). Congratulations, team!
              </li>
              <li>
                09 - 2024: Our work on building
                <span class="highlight">an open-source platform for real-time qubit data storage and visualization</span>
                is accepted to
                <a
                  target="_blank"
                  class="highlight"
                  href="https://www.nature.com/articles/s41598-024-72584-9.epdf?sharing_token=tnedWEyClF7jp8yoWcr8itRgN0jAjWel9jnR3ZoTv0O7cdSmeZlX_S9MZfBOrfYYa6i_99JuOMXKqu8qDDNUclCtVsSnujOueDL5OlSHsCGphUslu3Y3yS2QioRcDnaeHTWAz1eGFtg0w2AYPDE_gQWzmrG3rykBdfeLyzetzqc%3D"
                  >Nature Scientific Reports</a
                >. Congratulations, team!
              </li>
              <li>
                09 - 2024: Paper on 3D facial annimation for authentication is accepted to
                <span class="highlight">IEEE Trans. on Mobile Computing</span>. Congratulations, team!
              </li>
              <li>
                07 - 2024: We lead an <span class="highlight">NSF CSR award</span> focused on developing implantable and wearable sensors
                for plant health monitoring.
              </li>
              <li>
                06 - 2024: We have released a pre-print summarizing our two years of work on building an
                <span class="highlight">open-source ML-powered FPGA hardware/software platform</span> for in situ qubit control. We will
                discuss it at Quantum Week (QCE) 2024. More to come soon!
              </li>
              <li>
                06 - 2024: Welcome
                <b>Joseph Vo, Tanvi Kandepuneni, Michael Knox, Joseph Collins, Patrick Do, Gaurav Chandra, Aryan Nair, and Yuqi Liu</b>
                to the Summer Research Program at WSSL!
              </li>
              <li>
                05 - 2024: Our dear members, <b>Neel Vora</b> and <b>Devanshu Brahmbhatt</b>, graduated. Neel joined
                <b>
                  <a href="https://www.lbl.gov/">Berkeley Lab</a>
                </b>
                as a research staff. Devanshu continues his start-up dream. Congratulations!
              </li>
              <li>
                03 - 2024: Our
                <b>
                  <a href="https://www.a2collective.ai/">collaborative a2 pilot study</a>
                </b>
                with
                <b>
                  <a href="https://neursantys.com/">Neursantys</a>
                </b>
                is funded by
                <b>
                  <a href="https://www.nia.nih.gov/">National Institute on Aging</a>
                </b>
                via
                <b> <a href="https://massaitc.org/">MassAITC</a> </b>.
              </li>
              <li>
                02 - 2024: Prof. Nguyen gives an invited talk at
                <b>
                  <a href="https://www.dlsph.utoronto.ca/event/sustainable-head-worn-computers-design-development-and-deployment/"
                    >University of Toronto - Dalla Lana School of Public Health</a
                  >
                </b>
              </li>
              <li>
                01 - 2024: IOTeeth is accepted
                <b> <a href="https://www.ubicomp.org/ubicomp-iswc-2024/">ACM IMWUT/UbiComp 2024</a> </b>. Congratulations, Amir!
              </li>
              <li>
                01 - 2024: Our paper is accepted to
                <b> <a href="#">Journal of Clinical Neurophysiology</a> </b>. Congratulations, Aziz!
              </li>
              <li>
                01 - 2024: Prof. Nguyen gives an invited talk at
                <b> <a href="https://www.icoin.org/">ICOIN 2024</a> </b>.
              </li>
              <li>
                12 - 2023: Prof. Nguyen gives an invited talk at
                <b>
                  <a href="https://www.lbl.gov/">Lawrence Berkeley National Laboratory</a>
                </b>
                on ML + FPGA-based research and visit
                <b> <a href="https://www.berkeley.edu/">UCB</a> </b>.
              </li>
              <li>
                11 - 2023: Prof. Nguyen gives an invited talk at
                <b>
                  <a href="https://www.umassmed.edu/">UMass Chan Medical</a>
                </b>
                on earable health.
              </li>
              <li>
                10 - 2023: Two papers are accepted to
                <b>
                  <a href="https://sensys.acm.org/2023/">ACM SenSys 2023</a>
                </b>
                (Acceptance Ratio ~18.9%). Congratulations, teams!
              </li>
              <li>
                10 - 2023: Prof. Nguyen gives an invited talk at
                <b>
                  <a href="https://mortonarb.org/">The Morton Arboretum</a>
                </b>
                on the future of Internet of Trees.
              </li>
              <li>
                09 - 2023: Two papers are accepted to ACM
                <a href="https://sensys.acm.org/2023/">
                  <strong> SenSys 2023</strong>
                </a>
                (Acceptance Ratio ~18.9%).Congratulations, teams!
              </li>
              <li>
                09 - 2023: Congratulations Neel Vora for getting an Internship offer from
                <a href="https://www.lbl.gov/" target="_blank">
                  <strong>Lawrence Berkeley National Lab.</strong>
                </a>
                He will be in CA for Fall 2023. Cheers!
              </li>
              <li>
                08 - 2023: Congratulations Abdul Aziz for acceptance of
                <a href="./papers/Earables as Medical Devices.pdf" target="_blank">
                  <strong>Paper</strong>
                </a>
                at ACM EarComp 2023 in association with UbiComp/IMWUT 2023.
              </li>
              <li>
                08 - 2023: Congratulations Jackson Liller for acceptance of
                <a href="https://dl.acm.org/doi/10.1145/3614214.3614216" target="_blank">
                  <strong>Article</strong>
                </a>
                at GetMobile journal.
              </li>
              <li>
                07 - 2023: Congratulations team for acceptance of
                <a href="https://www.mdpi.com/2072-6694/15/14/3645" target="_blank">
                  <strong>paper</strong>
                </a>
                at Cancers journal.
              </li>
              <li>
                06 - 2023: Congratulations team for acceptance of paper
                <a href="https://dl.acm.org/doi/abs/10.1145/3597060.3597243" target="_blank">
                  <strong>Exploring Batteryless UAVs by Mimicking Bird Flight</strong>
                </a>
                at ACM DroNet'23
              </li>
              <li>
                06 - 2023: Congratulations Neel R Vora for acceptance of paper
                <a href="https://dl.acm.org/doi/10.1145/3597060.3597237" target="_blank">
                  <strong>DroneChase</strong>
                </a>
                at ACM DroNet'23
              </li>
              <li>
                06 - 2023: Congratulations Tasnim Azad Abir for acceptance of paper
                <a href="https://doi.org/10.1145/3597060.3597236" target="_blank">
                  <strong>Towards Robust Lidar-based 3D Detection and Tracking of UAVs</strong>
                </a>
                at ACM DroNet'23
              </li>
              <li>
                03 - 2023: Congratulations Devanshu Brahmbhatt for getting an Internship offer from
                <a href="https://www.lbl.gov/" target="_blank">
                  <strong>Lawrence Berkeley National Lab.</strong>
                </a>
                He will be in CA for Summer 2023. Cheers!
              </li>
              <li>
                01 - 2023: Jackson Spray joins
                <a href="https://lockheedmartin.com/en-us/index.html" target="_blank">
                  <strong>Lockheed Martin</strong>
                </a>
                as a Research Scientist. Congratulations, JD!
              </li>
              <li>
                09 - 2022: Our research is funded by
                <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2152357&HistoricalAwards=false"> <b> NSF ECCS</b> </a>. Thanks NSF
                for the support!
              </li>
              <li>
                08 - 2022: We are running
                <a href="http://wsslab.org/earable_tutorial_22/">
                  <b> Tutorial on Earable Computing</b>
                </a>
                at
                <a href="https://www.sigmobile.org/mobicom/2022/index.html"> <b> ACM MobiCom 2022</b> </a>. Please join us in Sydney!
              </li>
              <li>
                07 - 2022: MuteIt is accepted to
                <a href="https://ubicomp.org/ubicomp2022/"> <b> ACM UbiComp/IMUWT 2022</b> </a>. Congratulations, team!
              </li>
              <li>
                06 - 2022: Two papers (IoTree and PROS) are accepted to
                <a href="https://www.sigmobile.org/mobicom/2022/">
                  <b> ACM MobiCom 2022</b>
                </a>
                (Acceptance Ratio ~17.8%). Congratulations, teams!
              </li>
              <li>
                06 - 2022: Our workshop paper and demo are accepted to
                <a href="https://ipsn.acm.org/2022/index.html"> <b> MobiSys 2022</b> </a>.
              </li>
              <li>
                05 - 2022: Our poster at
                <a href="https://ipsn.acm.org/2022/index.html">
                  <b>IPSN 2022</b>
                </a>
                wins Best Poster Award. Congratulations, team!!!
              </li>
              <li>
                04 - 2022: Our MobiCom paper is awarded
                <a href="https://www.sigmobile.org/grav/publications/research-highlights"> <b>ACM SIGMOBILE Research Highlights 2022</b> </a
                >. Congratulations, team!!!
              </li>
              <li>
                04 - 2022: UCM covers our collaborative research on
                <a href="https://ucm.edu/YczPIS"> <b>Dental Health</b> </a>. Visit our demo at
                <a href="https://ipsn.acm.org/2022/index.html"> <b>IPSN 2022</b> </a>,
                <a href="https://cpsiotweek.neslab.it/"> <b>CPS-IoT Week 2022</b> </a>.
              </li>
              <li>
                03 - 2022: One poster and one demo are accepted to
                <a href="https://ipsn.acm.org/2022/"> <b>IPSN 2022</b> </a>.
              </li>
              <li>
                01 - 2022: UTA covers our
                <a href="https://www.uta.edu/news/news-releases/2022/01/13/nguyen-epilepsy-device">
                  <b>Epileptic Seizure Research</b>
                </a>
                with UT Southwestern and Oxford.
              </li>
              <li>
                10 - 2021: UTA covers our recent
                <a href="https://www.uta.edu/news/news-releases/2021/10/21/nguyen-vr-headset">
                  <b>NSF Award: Building a Better VR Headset</b> </a
                >.
              </li>
              <li>
                09 - 2021: Our paper is accepted to
                <a href="https://conferences.computer.org/chase2021/index.html"> <b>IEEE/ACM CHASE 2021</b> </a>.
              </li>
              <li>
                08 - 2021: Our project is funded by
                <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2132112&HistoricalAwards=false"> <b>NSF ECCS</b> </a>. Thanks,
                NSF, for the support!
              </li>
              <li>
                07 - 2021: We are extremely grateful to receive a set of Jetson Nano Developer Kits from
                <a href="https://developer.nvidia.com/blog/new-jetson-nano-2gb-developer-kit-grant-program-launches/">
                  <b>NVIDIA Grant Program</b> </a
                >.
              </li>
              <li>
                07 - 2021: Our COVID 19-related paper is accepted to
                <a href="https://www.ubicomp.org/ubicomp2021/"> <b>UbiComp (IMWUT) 2021</b> </a>. Congratulations, team
              </li>
              <li>
                06 - 2021: Prof. Nguyen received
                <a href="https://www.sony.com/electronics/research-award-program#FacultyInnovationAward">
                  <b>SONY Faculty Innovation Award</b> </a
                >. Thanks, SONY, for the support!
              </li>
              <li>
                06 - 2021: Our paper is conditionally accepted to
                <a href="https://sigmobile.org/mobicom/2021/">
                  <b>ACM MobiCom 2021.</b>
                </a>
              </li>
              <li>
                05 - 2021: Our microsleep (sleepiness) detection work is accepted to
                <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7755"> <b>IEEE Transactions on Mobile Computing</b> </a>.
              </li>
              <li>
                03 - 2021: Hosting
                <a href="http://wsslab.org/dronet21/"> <b>ACM DroNet 2021 at MobiSys 2021</b> </a>. Please consider submitting your papers!
              </li>
              <li>02 - 2021: Prof. Nguyen receives University of Texas System Rising Stars Award.</li>
              <li>
                12 - 2020: JawSense paper is accepted to
                <a href="http://www.hotmobile.org/2021/"> <b>ACM HotMobile 2021</b> </a>. Congratulations,
                <a href="http://wsslab.org/members.html#members"> <b>Tanmay and Prerna</b> </a>!
              </li>
              <li>
                12 - 2020: Prof. Nguyen is invited to serve in TPC of
                <a href="https://www.sigmobile.org/mobisys/2021/"> <b>ACM MobiSys 2021</b> </a>.
              </li>
              <li>
                11 - 2020: Prof. Nguyen serves in Panelists of
                <a href="http://sensys.acm.org/2020/dc/"> <b>ACM SenSys/BuildSys PhD Forum 2020</b> </a>.
              </li>
              <li>
                11 - 2020:
                <a href="./announcements/Undergraduate Research Scholar at WSSL.pdf">
                  <b>UNDERGRADUATE RESEARCH SCHOLAR POSITIONS ARE AVAILABLE</b>
                </a>
              </li>
              <li>
                10 - 2020: eBP is selected for publication in
                <a href="https://cacm.acm.org/about-communications/author-center/author-guidelines/about-research-highlights/">
                  <b>CACM Research Highlights 2020</b> </a
                >.
              </li>
              <li>
                10 - 2020: Profs. VP Nguyen and Shijia Pan published a positioning paper on wearable-infrastructure collaborative sensing at
                <a href="https://dfhs-buildsys.github.io/dfhs2020/index.html"> <b>ACM BuildSys-DFHS 2020 </b> </a>.
              </li>
              <li>
                09 - 2020: DroneScale paper is accepted to
                <a href="http://sensys.acm.org/2020/"> <b>ACM SenSys 2020</b> </a>. Congratulations, Vimal!
              </li>
              <li>
                I am looking for self-motivated students to work on research projects in mobile and wearable computing. Check my "
                <a href="members.html#joinus"> <b>Notes to Prospective Students</b> </a>" if you are interested in joining my lab.
              </li>
              <li>05 - 2020 - Lab website is online</li>
            </ul>
          </div>
        </div>
      </section>

      <section class="container anchor" id="projects">
        <hr class="half-margins" />
        <div class="row">
          <div>
            <h2>&emsp;Active <b>Projects (will be updated soon)</b></h2>
          </div>
        </div>
        <div class="row">
          <div class="col-md-12">
            <!-- <ul> -->
            <div class="row">
              <div class="col-md-5">
                <a href="index.html">
                  <img class="img-responsive img-hover" width="500" src="./assets/images/PROS_1.png" alt="" />
                  <img class="img-responsive img-hover" width="500" src="./assets/images/PROS.png" alt="" />
                </a>
              </div>
              <div class="col-md-6">
                <p>
                  <b>
                    <a href="./papers/pros 2022.pdf"
                      >PROS: an Efficient Pattern-Driven Compressive Sensing Framework for Low-Power Biopotential-based Wearables with
                      On-chip Intelligence</a
                    >
                  </b>
                  <br />
                  <a href="https://www.sigmobile.org/mobicom/2022/">
                    <b>ACM MobiCom 2022</b>
                  </a>
                  <font color="black"> .</font>
                  <br />
                  Nhat Pham, Hong Jia, Minh Tran, Tuan Dinh, Nam Bui, Young Kwon, Dong Ma, Phuc Nguyen, Cecilia Mascolo, and Tam Vu <br />
                </p>
                <h4>Abstract</h4>
                <p>
                  This study proposes PROS, an efficient pattern-driven compressive sensing framework for low-power biopotential-based
                  wearables. PROS eliminates the conventional trade-off between signal quality, response time, and power consumption by
                  introducing tiny pattern recognition primitives and a pattern-driven compressive sensing technique that exploits the
                  sparsity of biosignals. Specifically, we (i) develop tiny machine learning models to eliminate irrelevant biosignal
                  patterns, (ii) efficiently perform compressive sampling of relevant biosignals with appropriate sparse wavelet domains,
                  and (iii) optimize hardware and OS operations to push processing efficiency. PROS also provides an abstraction layer, so
                  the application only needs to care about detected relevant biosignal patterns without knowing the optimizations
                  underneath..
                </p>
              </div>
            </div>

            <hr />

            <div class="row">
              <div class="col-md-5">
                <a href="index.html">
                  <img class="img-responsive img-hover" width="500" src="./assets/images/IoTree_1.png" alt="" />
                  <br />
                  <iframe width="450" height="280" src="https://youtube.com/embed/fWm2NQ6EFNE" frameborder="0" allowfullscreen></iframe>
                </a>
              </div>
              <div class="col-md-6">
                <!-- <br/> -->
                <!-- <h3>WiKiSpiro</h3> -->
                <p>
                  <b>
                    <a href="./papers/iotree2022.pdf"
                      >IoTree: A Battery-free Wearable System with Biocompatible Sensors for Continuous Tree Health Monitoring</a
                    >
                  </b>
                  <br />
                  <a href="https://www.sigmobile.org/mobicom/2022/">
                    <b>ACM MobiCom 2022</b>
                  </a>
                  <font color="black"> .</font>
                  <br />
                  Tuan Dang, Trung Tran, Khang Nguyen, Tien Pham, Nhat Pham, Tam Vu, and Phuc Nguyen <br />
                </p>
                <h4>Abstract</h4>
                <p>
                  We present a low-maintenance, wind-powered, batteryfree, biocompatible, tree wearable, and intelligent sensing system,
                  namely IoTree, to monitor water and nutrient levels inside a living tree. IoTree system includes tiny-size, biocompatible,
                  and implantable sensors that continuously measure the impedance variations inside the living tree’s xylem, where water and
                  nutrients are transported from the root to the upper parts. The collected data are then compressed and transmitted to a
                  base station located at up to 1.8 kilometers (approximately 1.1 miles) away. The entire IoTree system is powered by wind
                  energy and controlled by an adaptive computing technique called block-based intermittent computing, ensuring the forward
                  progress and data consistency under intermittent power and allowing the firmware to execute with the most optimal memory
                  and energy usage. We prototype IoTree that opportunistically performs sensing, data compression, and long-range
                  communication tasks without batteries. During in-lab experiments, IoTree also obtains the accuracy of 91.08% and 90.51% in
                  measuring 10 levels of nutrients, 𝑁𝐻3 and 𝐾2𝑂, respectively. While tested with Burkwood Viburnum and White Bird trees in
                  the indoor environment, IoTree data strongly correlated with multiple watering and fertilizing events. We also deployed
                  IoTree on a grapevine farm for 30 days, and the system is able to provide sufficient measurements every day.
                </p>
              </div>
            </div>
            <hr />
            <div class="row">
              <div class="col-md-5">
                <a href="index.html">
                  <img class="img-responsive img-hover" width="500" src="./assets/images/Muteit.png" alt="" />
                </a>
              </div>
              <div class="col-md-6">
                <!-- <br/> -->
                <!-- <h3>WiKiSpiro</h3> -->
                <p>
                  <b>
                    <a href="./papers/MuteIt 2022.pdf">MuteIt: Jaw Motion based Unvoiced Command Recognition using Earable</a>
                  </b>
                  <br />
                  <a href="https://ubicomp.org/ubicomp2022/">
                    <b>ACM UbiComp/IMWUT 2022</b>
                  </a>
                  <font color="black"> .</font>
                  <br />
                  Tanmay Srivastava, Prerna Khanna, Shijia Pan, Phuc Nguyen, Shubham Jain <br />
                </p>
                <h4>Abstract</h4>
                <p>
                  we present MuteIt, an ear-worn system for recognizing unvoiced human commands. MuteIt presents an intuitive alternative to
                  voice-based interactions that can be unreliable in noisy environments, disruptive to those around us, and compromise our
                  privacy. We propose a twin-IMU set up to track the user's jaw motion and cancel motion artifacts caused by head and body
                  movements. MuteIt processes jaw motion during word articulation to break each word signal into its constituent syllables,
                  and further each syllable into phonemes (vowels, visemes, and plosives). Recognizing unvoiced commands by only tracking
                  jaw motion is challenging. As a secondary articulator, jaw motion is not distinctive enough for unvoiced speech
                  recognition. MuteIt combines IMU data with the anatomy of jaw movement as well as principles from linguistics, to model
                  the task of word recognition as an estimation problem. Rather than employing machine learning to train a word classifier,
                  we reconstruct each word as a sequence of phonemes using a bi-directional particle filter, enabling the system to be
                  easily scaled to a large set of words.
                </p>
              </div>
            </div>
            <hr />
            <div class="row">
              <div class="col-md-5">
                <a href="index.html">
                  <img class="img-responsive img-hover" width="500" src="./assets/images/FaceSense.png" alt="" />
                </a>
              </div>
              <div class="col-md-6">
                <p>
                  <b>
                    <a href="./papers/2021_Ubicomp_FaceSense.pdf">FaceSense: Sensing Face Touch with an Ear-worn System</a>
                  </b>
                  <br />
                  <a href="https://www.sigmobile.org/mobicom/2021/">
                    <b>ACM MobiCom 2021</b>
                  </a>
                  <font color="black"> .</font>
                  <br />
                  Vimal Kakaraparthi, Qijia Shao, Charles J. Carver, Tien Pham, Nam Bui, Phuc Nguyen, Xia Zhou, Tam Vu <br />
                </p>
                <h4>Abstract</h4>
                <p>
                  Face touch is an unconscious human habit. Frequent touching of sensitive/mucosal facial zones (eyes, nose, and mouth)
                  increases health risks by passing pathogens into the body and spreading diseases. Furthermore, accurate monitoring of face
                  touch is critical for behavioral intervention. Existing monitoring systems only capture objects approaching the face,
                  rather than detecting actual touches. As such, these systems are prone to false positives upon hand or object movement in
                  proximity to one’s face (e.g., picking up a phone). We present FaceSense, an ear-worn system capable of identifying actual
                  touches and differentiating them between sensitive/mucosal areas from other facial areas. Following a multimodal approach,
                  FaceSense integrates low-resolution thermal images and physiological signals. Thermal sensors sense the thermal infrared
                  signal emitted by an approaching hand, while physiological sensors monitor impedance changes caused by skin deformation
                  during a touch. Processed thermal and physiological signals are fed into a deep learning model (TouchNet) to detect
                  touches and identify the facial zone of the touch.
                </p>
              </div>
            </div>
            <hr />
            <div class="row">
              <div class="col-md-5">
                <a href="index.html">
                  <iframe
                    width="450"
                    height="280"
                    src="https://www.youtube.com/embed/54OJcwuDmOQ"
                    title="YouTube video player"
                    frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen
                  ></iframe>
                  <br />
                  <img class="img-responsive img-hover" width="500" src="./assets/images/BioFace.JPG" alt="" />
                </a>
              </div>
              <div class="col-md-6">
                <!-- <br/> -->
                <!-- <h3>WiKiSpiro</h3> -->
                <p>
                  <b>
                    <a href="./papers/2021_MobiCom_BioFace.pdf"
                      >BioFace-3D: Continuous 3D Facial Rconstruction Through Lightweight Single-ear Biosensors</a
                    >
                  </b>
                  <br />
                  <a href="https://www.sigmobile.org/mobicom/2021/">
                    <b>ACM MobiCom 2021</b>
                  </a>
                  <font color="black"> .</font>
                  <br />
                  Yi Wu, Vimal Kakaraparthi, Zhuohang Li, Tien Pham, Jian Liu, Phuc Nguyen <br />
                </p>
                <h4>Abstract</h4>
                <p>
                  We propose the first single-earpiece lightweight biosensing system, BioFace3D, that can unobtrusively, continuously, and
                  reliably sense the entire facial movements, track 2D facial landmarks, and further render 3D facial animations. Our
                  single-earpiece biosensing system takes advantage of the cross-modal transfer learning model to transfer the knowledge
                  embodied in a high-grade visual facial landmark detection model to the low-grade biosignal domain. After training, our
                  BioFace-3D can directly perform continuous 3D facial reconstruction from the biosignals, without any visual input. Without
                  requiring a camera positioned in front of the user, this paradigm shift from visual sensing to biosensing would introduce
                  new opportunities in many emerging mobile and IoT applications.
                </p>
              </div>
            </div>
            <hr />
            <div class="row">
              <div class="col-md-5">
                <a href="index.html">
                  <img class="img-responsive img-hover" width="500" src="./assets/images/DroneScale.png" alt="" />
                  <img class="img-responsive img-hover" width="500" src="./assets/images/DroneScale_1.png" alt="" />
                </a>
              </div>
              <div class="col-md-6">
                <!-- <br/> -->
                <!-- <h3>WiKiSpiro</h3> -->
                <p>
                  <b>
                    <a href="./papers/2020_DroneScale.pdf">DroneScale: drone load estimation via remote passive RF sensing</a>
                  </b>
                  <br />
                  <a href="https://www.sigmobile.org/mobicom/2021/">
                    <b>ACM MobiCom 2021</b>
                  </a>
                  <font color="black"> .</font>
                  <br />
                  Phuc Nguyen, Vimal Kakaraparthi, Nam Bui, Nikshep Umamahesh, Nhat Pham, Hoang Truong, Yeswanth Guddeti, Dinesh Bharadia,
                  Richard Han, Eric Frew, Daniel Massey, and Tam Vu <br />
                </p>
                <h4>Abstract</h4>
                <p>
                  Drones have carried weapons, drugs, explosives and illegal packages in the recent past, raising strong concerns from
                  public authorities. While existing drone monitoring systems only focus on detecting drone presence, localizing or
                  !ngerprinting the drone, there is a lack of a solution for estimating the additional load carried by a drone. In this
                  paper, we present a novel passive RF system, namely DroneScale, to monitor the wireless signals transmitted by commercial
                  drones and then con!rm their models and loads. Our key technical contribution is a proposed technique to passively capture
                  vibration at high resolution (i.e., 1Hz vibration) from afar, which was not possible before. We prototype DroneScale using
                  COTS RF components and illustrate that it can monitor the body vibration of a drone at the targeted resolution. In
                  addition, we develop learning algorithms to extract the physical vibration of the drone from the transmitted signal to
                  infer the model of a drone and the load carried by it. We evaluate the DroneScale system using 5 di"erent drone models,
                  which carry external loads of up to 400g..
                </p>
              </div>
            </div>
            <hr />
            <div class="row">
              <div class="row">
                <div class="col-md-5">
                  <a href="index.html">
                    <img class="img-responsive img-hover" width="500" src="./assets/images/painometry.jpg" alt="" />
                  </a>
                </div>
                <div class="col-md-6">
                  <!-- <br/> -->
                  <!-- <h3>WiKiSpiro</h3> -->
                  <p>
                    <b>
                      <a href="./papers/Painometry_2020.pdf"
                        >Painometry: Wearable and Objective Quantification System for Acute Postoperative Pain</a
                      >
                    </b>
                    <br />
                    <a href="https://www.sigmobile.org/mobisys/2020/">
                      <b>ACM MobiSys 2020</b>
                    </a>
                    <font color="black"> (34 out of 175 submissions, acceptance ratio: 19.4%).</font>
                    <br />
                    H. Truong, N. Bui, Z. Raghebi, M. Ceko, N. Pham, P. Nguyen, A. Nguyen, T. Kim, K. Siegfried, E. Stene, T. Tvrdy, L.
                    Weinman, T. Payne, D. Burke, T. Dinh, S. D’Mello, F. Banaei-Kashani, T. Wager, P. Goldstein, and T. Vu <br />
                  </p>
                  <h4>Abstract</h4>
                  <p>
                    This paper explores a wearable system, named Painometry, which objectively quanti!es users’ pain perception based-on
                    multiple physiological signals and facial expressions of pain. We propose a sensing technique, called sweep impedance
                    pro!ling (SIP), to capture the movement of the facial muscle corrugator supercilii, one of the important physiological
                    expressions of pain. We deploy SIP together with other biosignals, including electroencephalography (EEG),
                    photoplethysmogram (PPG), and galvanic skin response (GSR) for pain quanti!cation
                  </p>
                </div>
              </div>
            </div>
            <hr />

            <div class="row">
              <div class="col-md-5">
                <a href="index.html">
                  <iframe width="450" height="280" src="https://www.youtube.com/embed/6K-zi5BIqZ0" frameborder="0" allowfullscreen></iframe>
                </a>
              </div>
              <div class="col-md-6">
                <!-- <br/> -->
                <!-- <h3>WiKiSpiro</h3> -->
                <p>
                  <b>
                    <a href="./papers/WAKE_2020.pdf">WAKE: A Behind-the-ear Wearable System for Microsleep Detection</a>
                  </b>
                  <br />
                  <a href="https://www.sigmobile.org/mobisys/2020/">
                    <b>ACM MobiSys 2020</b>
                  </a>
                  <font color="black"> (34 out of 175 submissions, acceptance ratio: 19.4%).</font>
                  <br />
                  N. Pham, T. Dinh, Z. Raghebi, T. Kim, N. Bui, P. Nguyen, H. Truong, F. Banaei-Kashani, A. Halbower, T. Dinh, and T. Vu
                  <br />
                </p>
                <h4>Abstract</h4>
                <p>
                  We propose a novel behind-the-ear wearable device for microsleep detection, called WAKE. WAKE detects microsleep by
                  monitoring biosignals from the brain, eye movements, facial muscle contractions, and sweat gland activities from behind
                  the user’s ears. In particular, we introduce a Three-fold Cascaded Amplifying (3CA) technique to tame the motion artifacts
                  and environmental noises for capturing high fidelity signals. The behind-the-ear form factor is motivated by the fact that
                  bone-conductance headphones, which are worn around the ear, are becoming widely used. This technology trend gives us an
                  opportunity to enable a wide range of cognitive monitoring and improvement applications by integrating more sensing and
                  actuating functionality into the ear-phone, making it a smarter one.
                </p>
              </div>
            </div>

            <hr />
            <h4>
              <b>Please check our <a href="http://wsslab.org/publications.html">Publications</a> page for projects before 2020 </b>
            </h4>
            <br />
          </div>
        </div>
      </section>

      <section class="container anchor" id="projects">
        <div class="col-md-3">
          <h2>
            <b>Sponsors</b>
          </h2>
        </div>
        <div class="row" style="padding-bottom: 25px">
          <div class="col-md-12" id="sponsor-images">
            <img class="animate_fade_in" src="assets/images/nsf.png" alt="" />
            <img class="animate_fade_in" src="assets/images/sponsors/UMASS_Medium-TM.png" alt="" />
            <img class="animate_fade_in" src="assets/images/sponsors/cics.jpg" alt="" />
            <img class="animate_fade_in" src="assets/images/sponsors/nih_nia.png" alt="" />
            <img class="animate_fade_in" src="assets/images/sponsors/IALS.png" alt="" />
            <img class="animate_fade_in" src="assets/images/sponsors/sony.png" alt="" />
            <img class="animate_fade_in" src="assets/images/sponsors/UofTsystem_seal.svg" alt="" />
          </div>
          <style>
            /* Center the images horizontally */
            #sponsor-images {
              display: flex;
              justify-content: center; /* Centers the images horizontally */
              align-items: center; /* Centers the images vertically if there are varying heights */
              flex-wrap: wrap; /* Allows the images to wrap to the next line if they exceed the width */
              gap: 10px; /* Adds some spacing between images */
            }
          </style>
          <script>
            const imageWidth = 120;
            const images = document.querySelectorAll("#sponsor-images img");
            images.forEach((img) => {
              img.style.width = `${imageWidth}px`; // Dynamically set the width
            });
          </script>
        </div>
      </section>

      <footer>
        <div class="footer-content" \>
          <div class="container" \>
            <div class="footer-bar">
              <div class="container">
                <span class="copyright">Copyright &copy; Wireless and Sensor Systems Lab. All Rights Reserved.</span>
                <a class="toTop" href="#topNav">BACK TO TOP <i class="fa fa-arrow-circle-up"></i> </a>
              </div>
            </div>
          </div>
        </div>
      </footer>
    </div>
    <!-- JAVASCRIPT FILES -->
    <script type="text/javascript" src="assets/plugins/jquery-2.2.3.min.js"></script>
    <script type="text/javascript" src="assets/plugins/jquery.easing.1.3.js"></script>
    <script type="text/javascript" src="assets/plugins/jquery.cookie.js"></script>
    <script type="text/javascript" src="assets/plugins/jquery.appear.js"></script>
    <script type="text/javascript" src="assets/plugins/jquery.isotope.js"></script>
    <script type="text/javascript" src="assets/plugins/masonry.js"></script>
    <script type="text/javascript" src="assets/plugins/bootstrap/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="assets/plugins/magnific-popup/jquery.magnific-popup.min.js"></script>
    <script type="text/javascript" src="assets/plugins/owl-carousel/owl.carousel.min.js"></script>
    <script type="text/javascript" src="assets/plugins/stellar/jquery.stellar.min.js"></script>
    <script type="text/javascript" src="assets/plugins/knob/js/jquery.knob.js"></script>
    <script type="text/javascript" src="assets/plugins/jquery.backstretch.min.js"></script>
    <script type="text/javascript" src="assets/plugins/superslides/dist/jquery.superslides.min.js"></script>
    <script type="text/javascript" src="assets/plugins/styleswitcher/styleswitcher.js"></script>
    <!-- STYLESWITCHER - REMOVE ON PRODUCTION/DEVELOPMENT -->
    <script type="text/javascript" src="assets/plugins/mediaelement/build/mediaelement-and-player.min.js"></script>
    <!-- REVOLUTION SLIDER -->
    <script type="text/javascript" src="assets/plugins/slider.revolution.v4/js/jquery.themepunch.tools.min.js"></script>
    <script type="text/javascript" src="assets/plugins/slider.revolution.v4/js/jquery.themepunch.revolution.min.js"></script>
    <script type="text/javascript" src="assets/js/slider_revolution.js"></script>
    <script type="text/javascript" src="assets/js/scripts.js"></script>
  </body>
</html>
