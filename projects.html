<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Wireless and Sensor Systems Lab (WSSL)</title>
    <link rel="shortcut icon" href="assets/images/favicon/favicon.ico" type="image/x-icon" />
    <meta name="keywords" content="HTML5,CSS3,Template" />
    <meta name="description" content="" />
    <meta name="Author" content="Dorin Grigoras [www.stepofweb.com]" />
    <meta name="viewport" content="width=device-width, maximum-scale=1, initial-scale=1, user-scalable=0" />
    <link
      href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700,800"
      rel="stylesheet"
      type="text/css"
    />
    <link href="assets/plugins/bootstrap/css/bootstrap.min.css" rel="stylesheet" type="text/css" />
    <link href="assets/css/font-awesome.css" rel="stylesheet" type="text/css" />
    <link href="assets/plugins/owl-carousel/owl.carousel.css" rel="stylesheet" type="text/css" />
    <link href="assets/plugins/owl-carousel/owl.theme.css" rel="stylesheet" type="text/css" />
    <link href="assets/plugins/owl-carousel/owl.transitions.css" rel="stylesheet" type="text/css" />
    <link href="assets/plugins/magnific-popup/magnific-popup.css" rel="stylesheet" type="text/css" />
    <link href="assets/css/animate.css" rel="stylesheet" type="text/css" />
    <link href="assets/css/superslides.css" rel="stylesheet" type="text/css" />
    <link href="assets/plugins/slider.revolution.v4/css/settings.css" rel="stylesheet" type="text/css" />
    <link href="assets/css/essentials.css" rel="stylesheet" type="text/css" />
    <link href="assets/css/layout.css" rel="stylesheet" type="text/css" />
    <link href="assets/css/layout-responsive.css" rel="stylesheet" type="text/css" />
    <link href="assets/css/color_scheme/orange.css" rel="stylesheet" type="text/css" />
    <link
      href="assets/css/color_scheme/orange.css"
      rel="alternate stylesheet"
      type="text/css"
      title="orange"
    />
    <link href="assets/css/color_scheme/red.css" rel="alternate stylesheet" type="text/css" title="red" />
    <link href="assets/css/color_scheme/pink.css" rel="alternate stylesheet" type="text/css" title="pink" />
    <link
      href="assets/css/color_scheme/yellow.css"
      rel="alternate stylesheet"
      type="text/css"
      title="yellow"
    />
    <link
      href="assets/css/color_scheme/darkgreen.css"
      rel="alternate stylesheet"
      type="text/css"
      title="darkgreen"
    />
    <link href="assets/css/color_scheme/green.css" rel="alternate stylesheet" type="text/css" title="green" />
    <link
      href="assets/css/color_scheme/darkblue.css"
      rel="alternate stylesheet"
      type="text/css"
      title="darkblue"
    />
    <link href="assets/css/color_scheme/blue.css" rel="alternate stylesheet" type="text/css" title="blue" />
    <link href="assets/css/color_scheme/brown.css" rel="alternate stylesheet" type="text/css" title="brown" />
    <link
      href="assets/css/color_scheme/lightgrey.css"
      rel="alternate stylesheet"
      type="text/css"
      title="lightgrey"
    />
    <link href="assets/plugins/styleswitcher/styleswitcher.css" rel="stylesheet" type="text/css" />
    <script type="text/javascript" src="assets/plugins/modernizr.min.js"></script>
    <style>
      .card {
        /* Add shadows to create the "card" effect */
        box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2);
        transition: 0.3s;
      }

      /* On mouse-over, add a deeper shadow */
      .card:hover {
        box-shadow: 0 8px 16px 0 rgba(0, 0, 0, 0.2);
      }

      button {
        border: none;
        outline: 0;
        display: inline-block;
        padding: 8px;
        color: white;
        background-color: #000;
        text-align: center;
        cursor: pointer;
        width: 100%;
        font-size: 18px;
      }
    </style>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-169003278-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "UA-169003278-1");
    </script>
  </head>
  <body class="boxed pattern2">
    <header id="topNav">
      <div class="container">
        <div class="navbar-collapse nav-main-collapse collapse pull-left">
          <h2><b>Wireless and Sensor Systems</b> Lab (WSSL)</h2>
        </div>
        <div class="navbar-collapse nav-main-collapse collapse pull-right">
          <nav class="nav-main mega-menu">
            <ul class="nav nav-pills nav-main scroll-menu" id="topMain">
              <li class="dropdown active">
                <a class="dropdown-toggle" href="index.html"><b>Home</b></a>
              </li>
              <li class="dropdown mega-menu-item mega-menu-two-columns">
                <a class="dropdown-toggle" href="index.html#about"><b>About</b></a>
              </li>
              <li class="dropdown mega-menu-item mega-menu-fullwidth">
                <a class="dropdown-toggle" href="projects.html"><b>Projects</b></a>
              </li>
              <li class="dropdown">
                <a class="dropdown-toggle" href="publications.html"><b>Publications</b></a>
              </li>
              <li class="dropdown">
                <a class="dropdown-toggle" href="members.html#members"><b>Team</b></a>
              </li>
              <li class="dropdown">
                <a class="dropdown-toggle" href="members.html#joinus"><b>Join Us!</b></a>
              </li>
            </ul>
          </nav>
        </div>
      </div>
    </header>
    <span id="header_shadow"></span>
    <div id="wrapper">
      <section>
        <br />
        <div class="row">
          <div class="col-md-12">
            <br />
            <ul>
              <div class="row">
                <br />
                <br />
                <div class="col-md-5">
                  <a href="projects.html"
                    ><img
                      class="img-responsive img-hover"
                      width="500"
                      src="./assets/images/PROS_1.png"
                      alt="PROS_1" />
                    <img
                      class="img-responsive img-hover"
                      width="500"
                      src="./assets/images/PROS.png"
                      alt="PROS"
                  /></a>
                </div>
                <div class="col-md-6">
                  <p>
                    <b
                      ><a href="./papers/PROS_MobiCom_2022.pdf"
                        >PROS: an Efficient Pattern-Driven Compressive Sensing Framework for Low-Power
                        Biopotential-based Wearables with On-chip Intelligence</a
                      ></b
                    ><br />
                    <a href="https://www.sigmobile.org/mobicom/2022/"><b>ACM MobiCom 2022</b></a>
                    <font color="black">.</font><br />
                    Nhat Pham, Hong Jia, Minh Tran, Tuan Dinh, Nam Bui, Young Kwon, Dong Ma, Phuc Nguyen,
                    Cecilia Mascolo, and Tam Vu<br />
                  </p>
                  <h4>Abstract</h4>
                  <p>
                    This study proposes PROS, an efficient pattern-driven compressive sensing framework for
                    low-power biopotential-based wearables. PROS eliminates the conventional trade-off between
                    signal quality, response time, and power consumption by introducing tiny pattern
                    recognition primitives and a pattern-driven compressive sensing technique that exploits
                    the sparsity of biosignals. Specifically, we (i) develop tiny machine learning models to
                    eliminate irrelevant biosignal patterns, (ii) efficiently perform compressive sampling of
                    relevant biosignals with appropriate sparse wavelet domains, and (iii) optimize hardware
                    and OS operations to push processing efficiency. PROS also provides an abstraction layer,
                    so the application only needs to care about detected relevant biosignal patterns without
                    knowing the optimizations underneath..
                  </p>
                </div>
              </div>
              <hr />
              <div class="row">
                <div class="col-md-5">
                  <a href="projects.html"
                    ><img
                      class="img-responsive img-hover"
                      width="500"
                      src="./assets/images/IoTree_1.png"
                      alt="IoTree_1" /><br />
                    <iframe
                      width="400"
                      height="230"
                      src="https://youtube.com/embed/fWm2NQ6EFNE"
                      frameborder="0"
                      allowfullscreen
                    ></iframe
                  ></a>
                </div>
                <div class="col-md-6">
                  <p>
                    <b
                      ><a href="./papers/IoTree_MobiCom_2022.pdf"
                        >IoTree: A Battery-free Wearable System with Biocompatible Sensors for Continuous Tree
                        Health Monitoring</a
                      ></b
                    ><br />
                    <a href="https://www.sigmobile.org/mobicom/2022/"><b>ACM MobiCom 2022</b></a>
                    <font color="black">.</font><br />
                    Tuan Dang, Trung Tran, Khang Nguyen, Tien Pham, Nhat Pham, Tam Vu, and Phuc Nguyen<br />
                  </p>
                  <h4>Abstract</h4>
                  <p>
                    We present a low-maintenance, wind-powered, batteryfree, biocompatible, tree wearable, and
                    intelligent sensing system, namely IoTree, to monitor water and nutrient levels inside a
                    living tree. IoTree system includes tiny-size, biocompatible, and implantable sensors that
                    continuously measure the impedance variations inside the living tree’s xylem, where water
                    and nutrients are transported from the root to the upper parts. The collected data are
                    then compressed and transmitted to a base station located at up to 1.8 kilometers
                    (approximately 1.1 miles) away. The entire IoTree system is powered by wind energy and
                    controlled by an adaptive computing technique called block-based intermittent computing,
                    ensuring the forward progress and data consistency under intermittent power and allowing
                    the firmware to execute with the most optimal memory and energy usage. We prototype IoTree
                    that opportunistically performs sensing, data compression, and long-range communication
                    tasks without batteries. During in-lab experiments, IoTree also obtains the accuracy of
                    91.08% and 90.51% in measuring 10 levels of nutrients, 𝑁𝐻3 and 𝐾2𝑂, respectively. While
                    tested with Burkwood Viburnum and White Bird trees in the indoor environment, IoTree data
                    strongly correlated with multiple watering and fertilizing events. We also deployed IoTree
                    on a grapevine farm for 30 days, and the system is able to provide sufficient measurements
                    every day.
                  </p>
                </div>
              </div>
              <hr />
              <div class="row">
                <div class="col-md-5">
                  <a href="projects.html"
                    ><img
                      class="img-responsive img-hover"
                      width="500"
                      src="./assets/images/Muteit.png"
                      alt="Muteit"
                  /></a>
                </div>
                <div class="col-md-6">
                  <p>
                    <b
                      ><a href="./papers/2021_UbiComp_MuteIt.pdf"
                        >MuteIt: Jaw Motion based Unvoiced Command Recognition using Earable</a
                      ></b
                    ><br />
                    <a href="https://ubicomp.org/ubicomp2022/"><b>ACM UbiComp/IMWUT 2022</b></a>
                    <font color="black">.</font><br />
                    Tanmay Srivastava, Prerna Khanna, Shijia Pan, Phuc Nguyen, Shubham Jain<br />
                  </p>
                  <h4>Abstract</h4>
                  <p>
                    we present MuteIt, an ear-worn system for recognizing unvoiced human commands. MuteIt
                    presents an intuitive alternative to voice-based interactions that can be unreliable in
                    noisy environments, disruptive to those around us, and compromise our privacy. We propose
                    a twin-IMU set up to track the user's jaw motion and cancel motion artifacts caused by
                    head and body movements. MuteIt processes jaw motion during word articulation to break
                    each word signal into its constituent syllables, and further each syllable into phonemes
                    (vowels, visemes, and plosives). Recognizing unvoiced commands by only tracking jaw motion
                    is challenging. As a secondary articulator, jaw motion is not distinctive enough for
                    unvoiced speech recognition. MuteIt combines IMU data with the anatomy of jaw movement as
                    well as principles from linguistics, to model the task of word recognition as an
                    estimation problem. Rather than employing machine learning to train a word classifier, we
                    reconstruct each word as a sequence of phonemes using a bi-directional particle filter,
                    enabling the system to be easily scaled to a large set of words.
                  </p>
                </div>
              </div>
              <hr />
              <div class="row">
                <div class="col-md-5">
                  <a href="projects.html"
                    ><img
                      class="img-responsive img-hover"
                      width="500"
                      src="./assets/images/FaceSense.png"
                      alt="FaceSense"
                  /></a>
                </div>
                <div class="col-md-6">
                  <p>
                    <b
                      ><a href="./papers/2021_Ubicomp_FaceSense.pdf"
                        >FaceSense: Sensing Face Touch with an Ear-worn System</a
                      ></b
                    ><br />
                    <a href="https://www.sigmobile.org/mobicom/2021/"><b>ACM MobiCom 2021</b></a>
                    <font color="black">.</font><br />
                    Vimal Kakaraparthi, Qijia Shao, Charles J. Carver, Tien Pham, Nam Bui, Phuc Nguyen, Xia
                    Zhou, Tam Vu<br />
                  </p>
                  <h4>Abstract</h4>
                  <p>
                    Face touch is an unconscious human habit. Frequent touching of sensitive/mucosal facial
                    zones (eyes, nose, and mouth) increases health risks by passing pathogens into the body
                    and spreading diseases. Furthermore, accurate monitoring of face touch is critical for
                    behavioral intervention. Existing monitoring systems only capture objects approaching the
                    face, rather than detecting actual touches. As such, these systems are prone to false
                    positives upon hand or object movement in proximity to one’s face (e.g., picking up a
                    phone). We present FaceSense, an ear-worn system capable of identifying actual touches and
                    differentiating them between sensitive/mucosal areas from other facial areas. Following a
                    multimodal approach, FaceSense integrates low-resolution thermal images and physiological
                    signals. Thermal sensors sense the thermal infrared signal emitted by an approaching hand,
                    while physiological sensors monitor impedance changes caused by skin deformation during a
                    touch. Processed thermal and physiological signals are fed into a deep learning model
                    (TouchNet) to detect touches and identify the facial zone of the touch.
                  </p>
                </div>
              </div>
              <hr />
              <div class="row">
                <div class="col-md-5">
                  <a href="projects.html"
                    ><iframe
                      width="560"
                      height="315"
                      src="https://www.youtube.com/embed/54OJcwuDmOQ"
                      title="YouTube video player"
                      frameborder="0"
                      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                      allowfullscreen
                    ></iframe
                    ><br />
                    <img
                      class="img-responsive img-hover"
                      width="500"
                      src="./assets/images/BioFace.JPG"
                      alt="BioFace"
                  /></a>
                </div>
                <div class="col-md-6">
                  <p>
                    <b
                      ><a href="./papers/2021_MobiCom_BioFace.pdf"
                        >BioFace-3D: Continuous 3D Facial Rconstruction Through Lightweight Single-ear
                        Biosensors</a
                      ></b
                    ><br />
                    <a href="https://www.sigmobile.org/mobicom/2021/"><b>ACM MobiCom 2021</b></a>
                    <font color="black">.</font><br />
                    Yi Wu, Vimal Kakaraparthi, Zhuohang Li, Tien Pham, Jian Liu, Phuc Nguyen<br />
                  </p>
                  <h4>Abstract</h4>
                  <p>
                    We propose the first single-earpiece lightweight biosensing system, BioFace3D, that can
                    unobtrusively, continuously, and reliably sense the entire facial movements, track 2D
                    facial landmarks, and further render 3D facial animations. Our single-earpiece biosensing
                    system takes advantage of the cross-modal transfer learning model to transfer the
                    knowledge embodied in a high-grade visual facial landmark detection model to the low-grade
                    biosignal domain. After training, our BioFace-3D can directly perform continuous 3D facial
                    reconstruction from the biosignals, without any visual input. Without requiring a camera
                    positioned in front of the user, this paradigm shift from visual sensing to biosensing
                    would introduce new opportunities in many emerging mobile and IoT applications.
                  </p>
                </div>
              </div>
              <hr />
              <div class="row">
                <div class="col-md-5">
                  <a href="projects.html"
                    ><img
                      class="img-responsive img-hover"
                      width="500"
                      src="./assets/images/DroneScale.png"
                      alt="DroneScale" />
                    <img
                      class="img-responsive img-hover"
                      width="500"
                      src="./assets/images/DroneScale_1.png"
                      alt="DroneScale_1"
                  /></a>
                </div>
                <div class="col-md-6">
                  <p>
                    <b
                      ><a href="./papers/2020_DroneScale.pdf"
                        >DroneScale: drone load estimation via remote passive RF sensing</a
                      ></b
                    ><br />
                    <a href="https://www.sigmobile.org/mobicom/2021/"><b>ACM MobiCom 2021</b></a>
                    <font color="black">.</font><br />
                    Phuc Nguyen, Vimal Kakaraparthi, Nam Bui, Nikshep Umamahesh, Nhat Pham, Hoang Truong,
                    Yeswanth Guddeti, Dinesh Bharadia, Richard Han, Eric Frew, Daniel Massey, and Tam Vu<br />
                  </p>
                  <h4>Abstract</h4>
                  <p>
                    Drones have carried weapons, drugs, explosives and illegal packages in the recent past,
                    raising strong concerns from public authorities. While existing drone monitoring systems
                    only focus on detecting drone presence, localizing or !ngerprinting the drone, there is a
                    lack of a solution for estimating the additional load carried by a drone. In this paper,
                    we present a novel passive RF system, namely DroneScale, to monitor the wireless signals
                    transmitted by commercial drones and then con!rm their models and loads. Our key technical
                    contribution is a proposed technique to passively capture vibration at high resolution
                    (i.e., 1Hz vibration) from afar, which was not possible before. We prototype DroneScale
                    using COTS RF components and illustrate that it can monitor the body vibration of a drone
                    at the targeted resolution. In addition, we develop learning algorithms to extract the
                    physical vibration of the drone from the transmitted signal to infer the model of a drone
                    and the load carried by it. We evaluate the DroneScale system using 5 di"erent drone
                    models, which carry external loads of up to 400g..
                  </p>
                </div>
              </div>
              <hr />
              <div class="row">
                <div class="row">
                  <div class="col-md-5">
                    <a href="projects.html"
                      ><img
                        class="img-responsive img-hover"
                        width="500"
                        src="./assets/images/painometry.jpg"
                        alt="painometry"
                    /></a>
                  </div>
                  <div class="col-md-6">
                    <p>
                      <b
                        ><a href="./papers/Painometry_2020.pdf"
                          >Painometry: Wearable and Objective Quantification System for Acute Postoperative
                          Pain</a
                        ></b
                      ><br />
                      <a href="https://www.sigmobile.org/mobisys/2020/"><b>ACM MobiSys 2020</b></a>
                      <font color="black">(34 out of 175 submissions, acceptance ratio: 19.4%).</font><br />
                      H. Truong, N. Bui, Z. Raghebi, M. Ceko, N. Pham, P. Nguyen, A. Nguyen, T. Kim, K.
                      Siegfried, E. Stene, T. Tvrdy, L. Weinman, T. Payne, D. Burke, T. Dinh, S. D’Mello, F.
                      Banaei-Kashani, T. Wager, P. Goldstein, and T. Vu<br />
                    </p>
                    <h4>Abstract</h4>
                    <p>
                      This paper explores a wearable system, named Painometry, which objectively quanti!es
                      users’ pain perception based-on multiple physiological signals and facial expressions of
                      pain. We propose a sensing technique, called sweep impedance pro!ling (SIP), to capture
                      the movement of the facial muscle corrugator supercilii, one of the important
                      physiological expressions of pain. We deploy SIP together with other biosignals,
                      including electroencephalography (EEG), photoplethysmogram (PPG), and galvanic skin
                      response (GSR) for pain quanti!cation
                    </p>
                  </div>
                </div>
                <hr />
                <div class="row">
                  <div class="col-md-5">
                    <a href="projects.html"
                      ><iframe
                        width="400"
                        height="230"
                        src="https://www.youtube.com/embed/6K-zi5BIqZ0"
                        frameborder="0"
                        allowfullscreen
                      ></iframe
                    ></a>
                  </div>
                  <div class="col-md-6">
                    <p>
                      <b
                        ><a href="./papers/WAKE_2020.pdf"
                          >WAKE: A Behind-the-ear Wearable System for Microsleep Detection</a
                        ></b
                      ><br />
                      <a href="https://www.sigmobile.org/mobisys/2020/"><b>ACM MobiSys 2020</b></a>
                      <font color="black">(34 out of 175 submissions, acceptance ratio: 19.4%).</font><br />
                      N. Pham, T. Dinh, Z. Raghebi, T. Kim, N. Bui, P. Nguyen, H. Truong, F. Banaei-Kashani,
                      A. Halbower, T. Dinh, and T. Vu<br />
                    </p>
                    <h4>Abstract</h4>
                    <p>
                      We propose a novel behind-the-ear wearable device for microsleep detection, called WAKE.
                      WAKE detects microsleep by monitoring biosignals from the brain, eye movements, facial
                      muscle contractions, and sweat gland activities from behind the user’s ears. In
                      particular, we introduce a Three-fold Cascaded Amplifying (3CA) technique to tame the
                      motion artifacts and environmental noises for capturing high fidelity signals. The
                      behind-the-ear form factor is motivated by the fact that bone-conductance headphones,
                      which are worn around the ear, are becoming widely used. This technology trend gives us
                      an opportunity to enable a wide range of cognitive monitoring and improvement
                      applications by integrating more sensing and actuating functionality into the ear-phone,
                      making it a smarter one.
                    </p>
                  </div>
                </div>
                <hr />
                <h4>
                  <b
                    >Please check our <a href="http://wsslab.org/publications.html">Publications</a> page for
                    projects before 2020</b
                  >
                </h4>
                <br />
              </div>
            </ul>
          </div>
        </div>
      </section>
      <footer>
        <div class="footer-content">
          <div class="container">
            <div class="row">
              <div class="column logo col-md-4 text-center">
                <div class="logo-content">
                  <img
                    class="animate_fade_in"
                    src="assets/images/LOGO%20LAB-01.png"
                    width="350"
                    alt="LOGO%20LAB-01"
                  />
                </div>
              </div>
              <div class="column col-md-8">
                <h3>CONTACT</h3>
                <p class="contact-desc"></p>
                <address class="font-opensans">
                  <ul>
                    <li class="footer-sprite address">
                      <font color="white"
                        >Wireless and Sensor Systems Lab,<br />
                        The University of Texas at Arlington<br />
                        Engineering Research Building, 500 UTA Blvd<br />
                        Arlington, TX 76010<br
                      /></font>
                    </li>
                    <li class="footer-sprite phone"><font color="white">Phone: 817-272-2571</font></li>
                    <li class="footer-sprite email">
                      <a href="mailto:vp.nguyen@colorado.edu">vp.nguyen@uta.edu</a>
                    </li>
                  </ul>
                </address>
              </div>
            </div>
          </div>
        </div>
        <div class="footer-bar">
          <div class="container">
            <span class="copyright">Copyright © Wireless and Sensor Systems Lab. All Rights Reserved.</span>
            <a class="toTop" href="#topNav">BACK TO TOP</a>
          </div>
        </div>
      </footer>
      <script type="text/javascript" src="assets/plugins/jquery-2.2.3.min.js"></script>
      <script type="text/javascript" src="assets/plugins/jquery.easing.1.3.js"></script>
      <script type="text/javascript" src="assets/plugins/jquery.cookie.js"></script>
      <script type="text/javascript" src="assets/plugins/jquery.appear.js"></script>
      <script type="text/javascript" src="assets/plugins/jquery.isotope.js"></script>
      <script type="text/javascript" src="assets/plugins/masonry.js"></script>
      <script type="text/javascript" src="assets/plugins/bootstrap/js/bootstrap.min.js"></script>
      <script
        type="text/javascript"
        src="assets/plugins/magnific-popup/jquery.magnific-popup.min.js"
      ></script>
      <script type="text/javascript" src="assets/plugins/owl-carousel/owl.carousel.min.js"></script>
      <script type="text/javascript" src="assets/plugins/stellar/jquery.stellar.min.js"></script>
      <script type="text/javascript" src="assets/plugins/knob/js/jquery.knob.js"></script>
      <script type="text/javascript" src="assets/plugins/jquery.backstretch.min.js"></script>
      <script type="text/javascript" src="assets/plugins/superslides/dist/jquery.superslides.min.js"></script>
      <script type="text/javascript" src="assets/plugins/styleswitcher/styleswitcher.js"></script>
      <script
        type="text/javascript"
        src="assets/plugins/mediaelement/build/mediaelement-and-player.min.js"
      ></script>
      <script
        type="text/javascript"
        src="assets/plugins/slider.revolution.v4/js/jquery.themepunch.tools.min.js"
      ></script>
      <script
        type="text/javascript"
        src="assets/plugins/slider.revolution.v4/js/jquery.themepunch.revolution.min.js"
      ></script>
      <script type="text/javascript" src="assets/js/slider_revolution.js"></script>
      <script type="text/javascript" src="assets/js/scripts.js"></script>
    </div>
  </body>
</html>
